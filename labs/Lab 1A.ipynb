{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67609b90",
   "metadata": {},
   "source": [
    "# Lab Instructions\n",
    "\n",
    "In the lab, you're presented a task such as building a dataset, training a model, or writing a training loop, and we'll provide the code structured in such a way that you can fill in the blanks in the code using the knowledge you acquired in the chapters that precede the lab. You should be able to find appropriate snippets of code in the course content that work well in the lab with minor or no adjustments.\n",
    "\n",
    "The blanks in the code are indicated by ellipsis (`...`) and comments (`# write your code here`).\n",
    "\n",
    "In some cases, we'll provide you partial code to ensure the right variables are populated and any code that follows it runs accordingly.\n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "x = ...\n",
    "```\n",
    "\n",
    "The solution should be a single statement that replaces the ellipsis, such as:\n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "x = [0, 1, 2]\n",
    "```\n",
    "\n",
    "In some other cases, when there is no new variable being created, the blanks are shown like in the example below: \n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "...\n",
    "```\n",
    "\n",
    "Although we're showing you only a single ellipsis (`...`), you may have to write more than one line of code to complete the step, such as:\n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "for i, xi in enumerate(x):\n",
    "    x[i] = xi * 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3633df",
   "metadata": {
    "id": "7d922f0a"
   },
   "source": [
    "## 3.7 Lab 1A: Non-Linear Regression\n",
    "\n",
    "In this lab, you will use the same [Auto MPG Dataset](https://archive.ics.uci.edu/ml/datasets/auto+mpg), but we'll bring more features to the mix, as you will also learn how to encode discrete/categorical features so they can be used to train the model.\n",
    "\n",
    "The columns, or attributes, of this dataset, are as follows:\n",
    "\n",
    "1. mpg: continuous\n",
    "2. cylinders: multi-valued discrete\n",
    "3. displacement: continuous\n",
    "4. horsepower: continuous\n",
    "5. weight: continuous\n",
    "6. acceleration: continuous\n",
    "7. model year: multi-valued discrete\n",
    "8. origin: multi-valued discrete\n",
    "9. car name: string (unique for each instance)\n",
    "\n",
    "Remember that the last column, `car name`, is actually separated by tabs (instead of spaces), so we're considering the cars' names as comments while loading the dataset.\n",
    "\n",
    "We're loading the dataset into a Pandas dataframe just like before. Run the code below as is to load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5318baf2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "81f99d56",
    "outputId": "8c7b47a9-ba68-4e24-d853-8c366a62d28c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names = ['mpg', 'cyl', 'disp', 'hp', 'weight', 'acc', 'year', 'origin']\n",
    "\n",
    "df = pd.read_csv(url, names=column_names, na_values='?', comment='\\t', sep=' ', skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad02bd8",
   "metadata": {},
   "source": [
    "Just run the code below as is to visualize the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a864c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a7ff55",
   "metadata": {
    "id": "4c3ac779"
   },
   "source": [
    "### 3.7.1 Train-Validation-Test Split\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step1.png)\n",
    "\n",
    "Shuffle the dataset, and then split it into train, validation, and test sets using Scikit-Learn's `train_test_split()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ece4ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# write your code here\n",
    "shuffled = ...\n",
    "\n",
    "# write your code here\n",
    "raw_data = {}\n",
    "trainval, raw_data['test'] = ...\n",
    "raw_data['train'], raw_data['val'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9be2bf5",
   "metadata": {
    "id": "dcabaa3f"
   },
   "source": [
    "### 3.7.2 Cleaning Data\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step2.png)\n",
    "\n",
    "In this lab, we're throwing rows with missing values away, so make sure there are no NAs left in your datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752156af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f281a4",
   "metadata": {
    "id": "69fad9fe"
   },
   "source": [
    "### 3.7.3 Continuous Attributes\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step3.png)\n",
    "\n",
    "We've done this already, but this time you should write a `standardize()` function that:\n",
    "- takes a Pandas dataframe, a list of column names that are continuous attributes, and an optional scaler\n",
    "- creates and trains a Scikit-Learn's `StandardScaler` if one isn't provided as an argument\n",
    "- returns a PyTorch tensor containing the standardized features and an instance of Scikit-Learn's `StandardScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ec027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standardize(df, cont_attr, scaler=None):\n",
    "    # write your code here\n",
    "    ...\n",
    "    \n",
    "    # cont_X is a tensor containing the standardized features\n",
    "    # scaler is an instance of Scikit-Learn's StandardScaler\n",
    "    return cont_X, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491fede2",
   "metadata": {
    "id": "b1d06e9c"
   },
   "source": [
    "Use your `standardize` function to standardize all continuous attributes in our datasets. Don't forget you shouldn't train scalers on validation and test sets. They must use the scaler trained on the training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd23a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_attr = ['disp', 'hp', 'weight', 'acc']\n",
    "\n",
    "cont_data = {'train': None, 'val': None, 'test': None}\n",
    "\n",
    "# write your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a816d30",
   "metadata": {
    "id": "492d5d39"
   },
   "source": [
    "### 3.7.4 Categorical Attributes\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step3.png)\n",
    "\n",
    "Similary to the standardization function you already wrote, now write a function to encode categorical attributes:\n",
    "- takes a Pandas dataframe, a list of column names that are categorical attributes, and an optional encoder\n",
    "- creates and trains a Scikit-Learn's `OrdinalEncoder` if one isn't provided as an argument\n",
    "- returns a PyTorch tensor containing the encoded categorical features and an instance of Scikit-Learn's `OrdinalEncoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520d1240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "def encode(df, cat_attr, encoder=None):\n",
    "    # write your code here\n",
    "    ...\n",
    "    \n",
    "    return cat_X, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ae7c23",
   "metadata": {
    "id": "126ff94e"
   },
   "source": [
    "Use your `encode` function to encode all categorical attributes in our datasets. Don't forget you shouldn't train encoders on validation and test sets. They must use the encoder trained on the training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45c92df",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_attr = ['cyl', 'origin']\n",
    "\n",
    "cat_data = {'train': None, 'val': None, 'test': None}\n",
    "# write your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeda537",
   "metadata": {
    "id": "a8c557c1"
   },
   "source": [
    "The `categories_` attribute of the trained encoder should a list of lists of unique values, one list for each encoded attribute (just run the code below as is to visualize the output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67f3a2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f2255b7",
    "outputId": "f76ffc09-6f8e-4ac2-dee4-774bfdceeb8f"
   },
   "outputs": [],
   "source": [
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563ceaa5",
   "metadata": {
    "id": "612393dc"
   },
   "source": [
    "If we check the encoded attributes, their unique values should be lists of sequential numbers (just run the code below as is to visualize the output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd99d801",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2a04ce19",
    "outputId": "5dcf58c2-b088-4d0d-d598-a4f2ba882059"
   },
   "outputs": [],
   "source": [
    "cat_data['train'][:, 0].unique(), cat_data['train'][:, 1].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42f9e18",
   "metadata": {
    "id": "92deaced"
   },
   "source": [
    "### 3.7.5 Target and Task\n",
    "\n",
    "Your features are already taken care of, so it's time to create column tensors for your target attribute. Make sure they are of the type `float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748b6e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = {'train': None, 'val': None, 'test': None}\n",
    "target_col = 'mpg'\n",
    "\n",
    "# write your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcb58bb",
   "metadata": {
    "id": "10723837"
   },
   "source": [
    "### 3.7.6 Custom Dataset\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step4.png)\n",
    "\n",
    "Previously, we used a simple `TensorDataset` for our single feature and target. Now let's build our own custom dataset class instead by inheriting from the `Dataset` class. \n",
    "\n",
    "It needs to implement some basic methods:\n",
    "- `__init__(self)`\n",
    "- `__getitem__(self, index)`\n",
    "- `__len__(self)`. \n",
    "\n",
    "The constructor (`__init__()`) method may receive any arguments you can possible need, so you can create and preprocess your tensors right away or, as it is often the case when your dataset is too large, load them on demand. In our case, the constructor will receive the following arguments:\n",
    "\n",
    "- `raw_data`: a Pandas dataframe containing our (small) dataset\n",
    "- `cont_attr`: a list of the continuous attributes we'd like to use\n",
    "- `disc_attr`: a list of the discrete/categorical attributes we'd like to use\n",
    "- `target`: the name of the column containing the target attribute we'd like to predict\n",
    "- `scaler`: an optional instance of a `StandardScaler` to standardize the continuous attributes\n",
    "- `encoder`: an optional instance of an `OrdinalEncoder` to encode the discrete attributes sequentially\n",
    "\n",
    "You can use these arguments to preprocess and store the resulting tensors as class attributes, which you can retrieve at your convenience when other methods are called. Remember that you have already written functions to standardize continuous attributes and to encode categorical ones, feel free to use them.\n",
    "\n",
    "In the `__getitem__()` method, which makes a dataset \"sliceable\" just like a Python list, you should return a tuple `(features, target)` corresponding to the requested index. Notice that the first element of your tuple, `features` does not necessarily need to be a single tensor. It may be anything, another tuple, or even a dictionary. Remember that we have two types of features, continuous and categorical, and they are going to be handled differently in our model.\n",
    "\n",
    "In the `__len__()` method, you only need to return the total number of elements in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a41a680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, raw_data, cont_attr, disc_attr, target_col, scaler=None, encoder=None):\n",
    "        # write your code here\n",
    "        self.n = ...\n",
    "        self.target = ...\n",
    "        self.cont_data, self.scaler = ...\n",
    "        self.cat_data, self.encoder = ...\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # write your code here\n",
    "        features = ...\n",
    "        target = ...\n",
    "        return (features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0874a911",
   "metadata": {
    "id": "a9e3a3c2"
   },
   "source": [
    "Once your custom class has been defined, use it to create training, validation, and test datasets. Don't forget that scaling and encoding should be fitted in the training set only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e814ff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {'train': None, 'val': None, 'test': None}\n",
    "# write your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c9c0b7",
   "metadata": {},
   "source": [
    "Just run the code below as is to visualize the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e86492",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "af92e523",
    "outputId": "360f1740-9318-4513-e030-d4985c508e08"
   },
   "outputs": [],
   "source": [
    "datasets['train'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2130d0",
   "metadata": {
    "id": "d7919d44"
   },
   "source": [
    "You should see the features and targets of the first five elements from your training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db00e81",
   "metadata": {
    "id": "89150a50"
   },
   "source": [
    "### 3.7.7 Data Loaders\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step5.png)\n",
    "\n",
    "Next, you need to create data loaders, one for each set. It is recommended to shuffle the training set, but don't bother shuffling the others. Dropping the last mini-batch, in case your set isn't a perfect multiple of your mini-batch size, is also recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fefe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloaders = {'train': None, 'val': None, 'test': None}\n",
    "# write your code here\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
