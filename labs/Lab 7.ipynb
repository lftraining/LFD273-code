{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67609b90",
   "metadata": {},
   "source": [
    "# Lab Instructions\n",
    "\n",
    "In the lab, you're presented a task such as building a dataset, training a model, or writing a training loop, and we'll provide the code structured in such a way that you can fill in the blanks in the code using the knowledge you acquired in the chapters that precede the lab. You should be able to find appropriate snippets of code in the course content that work well in the lab with minor or no adjustments.\n",
    "\n",
    "The blanks in the code are indicated by ellipsis (`...`) and comments (`# write your code here`).\n",
    "\n",
    "In some cases, we'll provide you partial code to ensure the right variables are populated and any code that follows it runs accordingly.\n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "x = ...\n",
    "```\n",
    "\n",
    "The solution should be a single statement that replaces the ellipsis, such as:\n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "x = [0, 1, 2]\n",
    "```\n",
    "\n",
    "In some other cases, when there is no new variable being created, the blanks are shown like in the example below: \n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "...\n",
    "```\n",
    "\n",
    "Although we're showing you only a single ellipsis (`...`), you may have to write more than one line of code to complete the step, such as:\n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "for i, xi in enumerate(x):\n",
    "    x[i] = xi * 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4a6ad7",
   "metadata": {},
   "source": [
    "## Installation Notes\n",
    "\n",
    "To run this notebook on Google Colab, you will need to install the following libraries: transformers, sentence-transformers, and sec-edgar-downloader.\n",
    "\n",
    "In Google Colab, you can run the following command to install these libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c02fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sentence-transformers sec-edgar-downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb21ba19",
   "metadata": {
    "id": "cb21ba19"
   },
   "source": [
    "## 17.4 Lab 7: Document Q&A\n",
    "\n",
    "In this lab, we'll put together several tools we already used to extract information from a set of documents, also called \"Document Q&A\". We'll retrieve the latest 10-K forms filed by S&P500 top companies and search for information about their reported risks using natural language.\n",
    "\n",
    "Here are the tickers for the top 25 companies, as of June 2023. Just run the code below as is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70afb004",
   "metadata": {
    "id": "70afb004"
   },
   "outputs": [],
   "source": [
    "tickers = ['AAPL', 'MSFT', 'AMZN', 'NVDA', 'GOOGL', 'GOOG', 'META', 'TSLA', 'UNH', 'XOM', 'JPM',\n",
    "           'JNJ', 'V', 'LLY', 'PG', 'AVGO', 'MA', 'HD', 'MRK', 'CVX', 'PEP', 'ABBV', 'KO', 'COST']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb6e4b0",
   "metadata": {
    "id": "ebb6e4b0"
   },
   "source": [
    "### 17.4.1 EDGAR\n",
    "\n",
    "EDGAR is the Securities and Exchange Commission's (SEC) Eletronic Data Gathering, Analysis, and Retrieval (EDGAR) system.\n",
    "\n",
    "\"_[it] performs automated collection, validation, indexing, acceptance, and forwarding of submissions by companies and others who are required by law to file forms with the U.S. Securities and Exchange Commission (SEC). Its primary purpose is to increase the efficiency and fairness of the securities market for the benefit of investors, corporations, and the economy by accelerating the receipt, acceptance, dissemination, and analysis of time-sensitive corporate information filed with the agency._\"\n",
    "\n",
    "Source: [Important Information About EDGAR](https://www.sec.gov/edgar/searchedgar/aboutedgar.htm)\n",
    "\n",
    "### 17.4.2 Form 10-K\n",
    "\n",
    "In this lab, we'll be retrieving the latest 10-K form filed by the companies previously listed.\n",
    "\n",
    "\"_A Form 10-K is an annual report required by the U.S. Securities and Exchange Commission (SEC), that gives a comprehensive summary of a company's financial performance. Although similarly named, the annual report on Form 10-K is distinct from the often glossy \"annual report to shareholders,\" which a company must send to its shareholders when it holds an annual meeting to elect directors (though some companies combine the annual report and the 10-K into one document). The 10-K includes information such as company history, organizational structure, executive compensation, equity, subsidiaries, and audited financial statements, among other information._\"\n",
    "\n",
    "Source: [Wikipedia](https://en.wikipedia.org/wiki/Form_10-K)\n",
    "\n",
    "We'll be paying special attention to the section [\"Item 1A - Risk Factors\"](https://en.wikipedia.org/wiki/Form_10-K#Item_1A_%E2%80%93_Risk_Factors), where \"_...the company lays anything that could go wrong, likely external effects, possible future failures to meet obligations, and other risks disclosed to adequately warn investors and potential investors._\"\n",
    "\n",
    "### 17.4.3 Downloader\n",
    "\n",
    "While it's possible to retrieve public information directly from EDGAR, you'd have to find the proper identification numbers of companies and filings to download the reports. It is more conveniente to use a Python package that handles the nitty-gritty details for us and retrieves as many reports as we want by simply specifying the company's ticker (e.g. MSFT, GOOGL), and the type of report (e.g. 10-K). The package [`sec-edgar-downloader`](https://github.com/jadchaar/sec-edgar-downloader) does exactly that.\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step1.png)\n",
    "\n",
    "We can easily download the forms by creating an instance of a `Downloader` that points to the destination folder where files will be stored, and calling its `get()` method repeatedly, once for each ticker. Just run the code below as is to get the latest version of all reports.\n",
    "\n",
    "For the first time you run the code in this lab, though, we recommend you download the dataset we prepared for you, so you can more easily follow along and double-check your answers later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae72256",
   "metadata": {
    "id": "eae72256"
   },
   "outputs": [],
   "source": [
    "from sec_edgar_downloader import Downloader\n",
    "\n",
    "dest_folder = \"./edgar10k_sp500_top25\"\n",
    "dl = Downloader(\"MyCompanyName\", \"my.email@domain.com\", dest_folder)\n",
    "\n",
    "form = '10-K'\n",
    "for ticker in tickers:\n",
    "    dl.get(\"10-K\", ticker, limit=1, download_details=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ac167",
   "metadata": {
    "id": "477ac167"
   },
   "source": [
    "Alternatively, in order to get the same results as shown in this lab, you can download the compressed folder containing all forms (as of June 2023) from the following link:\n",
    "\n",
    "```\n",
    "https://github.com/dvgodoy/assets/releases/download/dataset/edgar10k_sp500_top25.tar.gz\n",
    "```\n",
    "\n",
    "You should uncompress the file, and rename the `filings` folder to `edgar10k_sp500_top25`.\n",
    "\n",
    "If you're using Google Colab, you may run the following commands to accomplish that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea4b3dc",
   "metadata": {
    "id": "cea4b3dc"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/dvgodoy/assets/releases/download/dataset/edgar10k_sp500_top25.tar.gz\n",
    "!tar -xvzf edgar10k_sp500_top25.tar.gz\n",
    "!mv filings edgar10k_sp500_top25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3938a81",
   "metadata": {
    "id": "a3938a81"
   },
   "source": [
    "It will create a subfolder for each ticker, each containing a folder corresponding to the downloaded form (10-K), and yet another folder named after the form's corresponding ID number. \n",
    "\n",
    "In the compressed dataset above, the inner folder is named `0001564590-22-026876` and it has two files: `filing-details.html` and `full-submission.txt`. If you're using Google Colab, you can run the command below to list the files inside that folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff7e4e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bff7e4e4",
    "outputId": "ab9a7ab1-b376-4a8d-ceb5-7f8ae53a8076"
   },
   "outputs": [],
   "source": [
    "!ls -l edgar10k_sp500_top25/sec-edgar-filings/MSFT/10-K/0001564590-22-026876"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e6c1d",
   "metadata": {},
   "source": [
    "We'll be using the details file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb57c2e3",
   "metadata": {
    "id": "bb57c2e3"
   },
   "source": [
    "### 17.4.4 Parser\n",
    "\n",
    "The details file is a mix of HTML and XML tags, and it would be very cumbersome to parse them ourselves. Fortunately, we can easily adapt a parser function, [`parse_10k_filing()`](https://github.com/rsljr/edgarParser/blob/master/parse_10K.py) from the [edgarParser](https://github.com/rsljr/edgarParser) repository, to parse our downloaded files.\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step2.png)\n",
    "\n",
    "Its original docstring states:\n",
    "\n",
    "\"_The function *parse_10k_filing()* parses 10-K forms to extract the following sections: business description, business risk, and management discussioin and analysis. The function takes two arguments, a link and a number indicating the section, and returns a list with the requested sections. Current options are **0(All), 1(Business), 2(Risk), 4(MDA).**_\"\n",
    "\n",
    "We'll be using option number two to retrieve text related to section \"Item 1A - Risk Factors\" only. Just run the code below as is to define the function we'll be using to parse the forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd391a9",
   "metadata": {
    "id": "dfd391a9"
   },
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/rsljr/edgarParser/blob/master/parse_10K.py\n",
    "import re\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "def parse_10k_filing(content, section):\n",
    "\n",
    "    if section not in [0, 1, 2, 3]:\n",
    "        print(\"Not a valid section\")\n",
    "        sys.exit()\n",
    "\n",
    "    def get_text(content):\n",
    "        html = bs(content, \"html.parser\")\n",
    "        text = html.get_text()\n",
    "        text = unicodedata.normalize(\"NFKD\", text).encode('ascii', 'ignore').decode('utf8')\n",
    "        text = text.split(\"\\n\")\n",
    "        text = \" \".join(text)\n",
    "        return(text)\n",
    "\n",
    "    def extract_text(text, item_start, item_end):\n",
    "        item_start = item_start\n",
    "        item_end = item_end\n",
    "        starts = [i.start() for i in item_start.finditer(text)]\n",
    "        ends = [i.start() for i in item_end.finditer(text)]\n",
    "        positions = list()\n",
    "        for s in starts:\n",
    "            control = 0\n",
    "            for e in ends:\n",
    "                if control == 0:\n",
    "                    if s < e:\n",
    "                        control = 1\n",
    "                        positions.append([s,e])\n",
    "        item_length = 0\n",
    "        item_position = list()\n",
    "        for p in positions:\n",
    "            if (p[1]-p[0]) > item_length:\n",
    "                item_length = p[1]-p[0]\n",
    "                item_position = p\n",
    "\n",
    "        item_text = text[item_position[0]:item_position[1]]\n",
    "\n",
    "        return(item_text)\n",
    "\n",
    "    text = get_text(content)\n",
    "\n",
    "    if section == 1 or section == 0:\n",
    "        try:\n",
    "            item1_start = re.compile(\"item\\s*[1][\\.\\;\\:\\-\\_]*\\s*\\\\b\", re.IGNORECASE)\n",
    "            item1_end = re.compile(\"item\\s*1a[\\.\\;\\:\\-\\_]\\s*Risk|item\\s*2[\\.\\,\\;\\:\\-\\_]\\s*Prop\", re.IGNORECASE)\n",
    "            businessText = extract_text(text, item1_start, item1_end)\n",
    "        except:\n",
    "            businessText = \"Something went wrong!\"\n",
    "\n",
    "    if section == 2 or section == 0:\n",
    "        try:\n",
    "            item1a_start = re.compile(\"(?<!,\\s)item\\s*1a[\\.\\;\\:\\-\\_]\\s*Risk\", re.IGNORECASE)\n",
    "            item1a_end = re.compile(\"item\\s*2[\\.\\;\\:\\-\\_]\\s*Prop|item\\s*[1][\\.\\;\\:\\-\\_]*\\s*\\\\b\", re.IGNORECASE)\n",
    "            riskText = extract_text(text, item1a_start, item1a_end)\n",
    "        except:\n",
    "            riskText = \"Something went wrong!\"\n",
    "\n",
    "    if section == 3 or section == 0:\n",
    "        try:\n",
    "            item7_start = re.compile(\"item\\s*[7][\\.\\;\\:\\-\\_]*\\s*\\\\bM\", re.IGNORECASE)\n",
    "            item7_end = re.compile(\"item\\s*7a[\\.\\;\\:\\-\\_]\\sQuanti|item\\s*8[\\.\\,\\;\\:\\-\\_]\\s*\", re.IGNORECASE)\n",
    "            mdaText = extract_text(text, item7_start, item7_end)\n",
    "        except:\n",
    "            mdaText = \"Something went wrong!\"\n",
    "\n",
    "    if section == 0:\n",
    "        data = [businessText, riskText, mdaText]\n",
    "    elif section == 1:\n",
    "        data = [businessText]\n",
    "    elif section == 2:\n",
    "        data = [riskText]\n",
    "    elif section == 3:\n",
    "        data = [mdaText]\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcc2b46",
   "metadata": {
    "id": "8fcc2b46"
   },
   "source": [
    "Let's parse the latest 10-K form filed by Microsoft (as of June 2023). Just run the code below as is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f60315",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50f60315",
    "outputId": "a4906ec9-1a81-493c-b7e1-83b931521df8"
   },
   "outputs": [],
   "source": [
    "with open('./edgar10k_sp500_top25/sec-edgar-filings/MSFT/10-K/0001564590-22-026876/filing-details.html', 'r',\n",
    "          encoding='utf-8') as f:\n",
    "    html = f.read()\n",
    "\n",
    "res = parse_10k_filing(html, 2)[0]\n",
    "len(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9315c9",
   "metadata": {},
   "source": [
    "That's about 70,000 characters. We need to split it into more manageable chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a5898",
   "metadata": {
    "id": "b55a5898"
   },
   "source": [
    "In Chapter 14, we briefly discussed chunking strategies. Sometimes, as in the case of our 10-K form filed by Microsoft, there's some other indication to the text's structure: it looks like paragraphs are separated by a sequence of two or more spaces.\n",
    "\n",
    "Let's try it out. Just run the code below as is to visualize the first three chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e8137f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67e8137f",
    "outputId": "2e0c5e10-b611-4856-d5d1-843b012939f2"
   },
   "outputs": [],
   "source": [
    "docs = res.split('  ')\n",
    "docs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46797ac",
   "metadata": {
    "id": "e46797ac"
   },
   "source": [
    "Looks good, these are definitely paragraphs. Unfortunately, this may not be the case for every document: in some 10-K forms, there's no clear indication of a paragraph, and you'll need to rely on a different chunking strategy to move forward. For now, we're sticking with this particular 10-K form, and we'll proceed using paragraphs as chunks.\n",
    "\n",
    "If we look at the full list, though, we'll see that there are many empty lines as well as really short ones that are likely section headers. We can discard these chunks that are too short (say, less than 10 characters long). Just run the code below as is to split the whole text into paragraphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccd72c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ccd72c9",
    "outputId": "ffdd725b-c510-410d-b8c6-81c688027fcd"
   },
   "outputs": [],
   "source": [
    "paragraphs = list(map(lambda s: s.strip(), filter(lambda s: len(s) > 10, res.split('  '))))\n",
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd4242c",
   "metadata": {
    "id": "7bd4242c"
   },
   "source": [
    "We got 88 paragraphs. Let's take a look at one of those paragraphs. Just run the code below as is to visualize the first paragraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889a0db1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "889a0db1",
    "outputId": "f6b0da2a-8d65-4d36-b6c1-5e0cfe280d8b"
   },
   "outputs": [],
   "source": [
    "text = paragraphs[1]\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bd78d8",
   "metadata": {
    "id": "d1bd78d8"
   },
   "source": [
    "How many words is that? Just run the code below as is to find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b89f58b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b89f58b",
    "outputId": "a5f93355-6025-42ab-e8d0-551dcbc1b45e"
   },
   "outputs": [],
   "source": [
    "len(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ce9b6d",
   "metadata": {
    "id": "82ce9b6d"
   },
   "source": [
    "Perhaps we can make it shorter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d124c7f6",
   "metadata": {
    "id": "d124c7f6"
   },
   "source": [
    "### 17.4.6 Summarization\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step5.png)\n",
    "\n",
    "Load a pretrained summarization pipeline from HuggingFace and use it to summarize the text above. Try different minimum and maximum lengths and observe the resulting summaries. How they compare to the original text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68341371",
   "metadata": {
    "id": "68341371"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# write your code here\n",
    "summarizer = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d037fae",
   "metadata": {},
   "source": [
    "Once you created your summarizer, just run the code below as is to summarize the text in the first paragraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da40f2e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "da40f2e1",
    "outputId": "a4742d22-3c03-40ea-b944-ea249d9887e0"
   },
   "outputs": [],
   "source": [
    "summarizer(text, max_length=50, min_length=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff65b3b",
   "metadata": {
    "id": "bff65b3b"
   },
   "source": [
    "Summarizing text is great, but we may be doing it prematurely at this point. Instead of summarizing individual paragraphs (or other chunks of text), it may be more interesting to find (full) paragraphs of interest first, and only then summarize them as a whole.\n",
    "\n",
    "If we're doing document Q&A, we need to query our documents (paragraphs) and find those that are more likely to contain the answer, that is, those more closely related to the topic of our query.\n",
    "\n",
    "How can we search for similar documents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd5cf1f",
   "metadata": {
    "id": "3dd5cf1f"
   },
   "source": [
    "### 17.4.7 Embeddings\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step3.png)\n",
    "\n",
    "You already know how to search for similar documents. You need to embed them first!\n",
    "\n",
    "Use the `sentence transformers` package to load a pretrained model for sentence embeddings (e.g. `all-MiniLM-L12-v2`) and embed every paragraph of text from Microsoft's 10-K form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb300be",
   "metadata": {
    "id": "ffb300be"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# write your code here\n",
    "model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2790dba4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2790dba4",
    "outputId": "bbeab982-1abc-4d32-98d7-4fb65e2989e4"
   },
   "outputs": [],
   "source": [
    "# write your code here\n",
    "embeddings = ...\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648b7cce",
   "metadata": {
    "id": "648b7cce"
   },
   "source": [
    "### 17.4.8 Searching\n",
    "\n",
    "There are two alternatives to search for similar embeddings, and we have tried them both already: PyTorch's own cosine similarity, and vector databases such as ChromaDB.\n",
    "\n",
    "At this point, let's keep it as simple as it can be, and stick with cosine similarity. Create an instance of the cosine similarity layer and use it to find five paragraphs that are most similar to the query below (don't forget to embed the query as well):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a14d98",
   "metadata": {
    "id": "c9a14d98"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# write your code here\n",
    "similarity = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f455a7",
   "metadata": {
    "id": "44f455a7"
   },
   "outputs": [],
   "source": [
    "# Embed the query and make it a tensor\n",
    "\n",
    "query = \"what are the sources of uncertainties?\"\n",
    "# write your code here\n",
    "q = ...\n",
    "content = torch.as_tensor(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3240bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0c3240bf",
    "outputId": "d01284c8-ba89-40f8-f3e5-d7070ba8e4df"
   },
   "outputs": [],
   "source": [
    "# Compute the cosine similarity between query and content\n",
    "# and get the top 5 results\n",
    "# write your code here\n",
    "similarities = ...\n",
    "most = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192342d8",
   "metadata": {
    "id": "192342d8"
   },
   "source": [
    "You should get a list of five indices corresponding to the paragraphs that are most relevant to our query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c2bca1",
   "metadata": {
    "id": "00c2bca1"
   },
   "source": [
    "### 17.4.9 Context\n",
    "\n",
    "Now, join all the paragraphs together as a single piece of text. This is going to be what is referred to as the \"context\". Notice that the indices may be ordered according to their similarity to the query. However, it's probably a good idea to order them as they appear on the text instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6789a43f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6789a43f",
    "outputId": "2d238c72-c222-4a1e-8f0c-47f8fef1df24"
   },
   "outputs": [],
   "source": [
    "# write your code here\n",
    "context = ...\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db4a7b",
   "metadata": {
    "id": "f2db4a7b"
   },
   "source": [
    "The context should contain the relevant information to answer our query, and it is one of the arguments you need to pass to a question answering pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e0162d",
   "metadata": {
    "id": "c1e0162d"
   },
   "source": [
    "### 17.4.10 Question Answering\n",
    "\n",
    "We have a question, \"_what are the sources of uncertainties?_\" and we have a context, five paragraphs from our text that are the most similar to the question. That's everything you need to try a question answering pipeline!\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step5.png)\n",
    "\n",
    "Create an instance of Q&A pipeline, and call it using its `question` and `context` arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e8895",
   "metadata": {
    "id": "a02e8895",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# write your code here\n",
    "qa_model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b62622",
   "metadata": {},
   "source": [
    "Once you created your Q&A pipeline, just run the code below as is to answer the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e90233",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9e90233",
    "outputId": "e08310a6-a3b1-42e2-aa7b-f1e171c1a670"
   },
   "outputs": [],
   "source": [
    "query = \"what are the sources of uncertainties?\"\n",
    "\n",
    "qa_model(question=query, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4482b5d8",
   "metadata": {
    "id": "4482b5d8"
   },
   "source": [
    "The Q&A model is good at answering questions that are extractive in nature and can be easily pinpointed in the text. It gives you back the start and end positions in the text that contain the answer to your question.\n",
    "\n",
    "It may technically correct, but perhaps it's a bit too short, right?\n",
    "\n",
    "In theory, the context should contain the relevant information to our query. But, it is too verbose and it doesn't read well, after all, it is just a sequence of paragraphs patched together. One way of trying to make it look more like an answer is to summarize it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3919d3",
   "metadata": {
    "id": "8a3919d3"
   },
   "source": [
    "Use the summarization pipeline you already created to summarize the context above. Make sure the minimum and maximum length are appropriate given the original length of the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf8aeb8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cf8aeb8",
    "outputId": "1dd84ba2-a38d-4dcf-c42f-0806f516306d"
   },
   "outputs": [],
   "source": [
    "# write your code here\n",
    "summary = ...\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161b723",
   "metadata": {
    "id": "e161b723"
   },
   "source": [
    "How do you like the summary? Does it look like an answer to our question? Could it have been better? Pause and ponder for a while, what could you do to get a better answer or, better yet, to get a better context back?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
