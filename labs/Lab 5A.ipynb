{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67609b90",
   "metadata": {},
   "source": [
    "# Lab Instructions\n",
    "\n",
    "In the lab, you're presented a task such as building a dataset, training a model, or writing a training loop, and we'll provide the code structured in such a way that you can fill in the blanks in the code using the knowledge you acquired in the chapters that precede the lab. You should be able to find appropriate snippets of code in the course content that work well in the lab with minor or no adjustments.\n",
    "\n",
    "The blanks in the code are indicated by ellipsis (`...`) and comments (`# write your code here`).\n",
    "\n",
    "In some cases, we'll provide you partial code to ensure the right variables are populated and any code that follows it runs accordingly.\n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "x = ...\n",
    "```\n",
    "\n",
    "The solution should be a single statement that replaces the ellipsis, such as:\n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "x = [0, 1, 2]\n",
    "```\n",
    "\n",
    "In some other cases, when there is no new variable being created, the blanks are shown like in the example below: \n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "...\n",
    "```\n",
    "\n",
    "Although we're showing you only a single ellipsis (`...`), you may have to write more than one line of code to complete the step, such as:\n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "for i, xi in enumerate(x):\n",
    "    x[i] = xi * 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c5e9f",
   "metadata": {
    "id": "888c5e9f"
   },
   "source": [
    "## 11.8 Lab 5A: Fine-Tuning Object Detection Models\n",
    "\n",
    "In this lab, you'll build a dataset, including data augmentation, and fine-tune a custom object detection model by replacing its standard backbone with a different computer vision model. In the end, you'll evaluate the model using metrics from the COCO challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce02517",
   "metadata": {
    "id": "3ce02517"
   },
   "source": [
    "### 11.8.1 Oxford-IIIT Pet Dataset\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step1.png)\n",
    "\n",
    "You'll build a dataset using the images and annotations from the [Oxford-IIIT Pet dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/):\n",
    "\n",
    "\"_We have created a 37 category pet dataset with roughly 200 images for each class. The images have a large variations in scale, pose and lighting. All images have an associated ground truth annotation of breed, head ROI, and pixel level trimap segmentation._\"\n",
    "\n",
    "You will load the data using [PyTorch's built-in class](https://pytorch.org/vision/stable/generated/torchvision.datasets.OxfordIIITPet.html), but you're tasked with preprocessing the annotations and building a dataset that is compatible with V2 transforms for data augmentation (without wrapping the built-in dataset, that is).\n",
    "\n",
    "First, load the data to the a folder of your choice (e.g. `./pets`), making sure to retrieve the `trainval` split (which has annotations), and choose both target types, `category` and `segmentation`, since you'll be fine-tuning a model to detect pets on images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432568a3",
   "metadata": {
    "id": "432568a3"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import OxfordIIITPet\n",
    "\n",
    "root_folder = './pets'\n",
    "# write the arguments to create an instance of the dataset\n",
    "pets = OxfordIIITPet(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a68cf78",
   "metadata": {
    "id": "2a68cf78"
   },
   "source": [
    "### 11.8.2 Annotations\n",
    "\n",
    "The annotations follow the Pascal VOC challenge format, and are stored as individual XML files, one for each annotated image, inside the `oxford-iiit-pet/annotations/xmls` subfolder. Use the `xml_to_csv()` helper function to convert all these files into a Pandas dataframe and inspect its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364ab0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def xml_to_csv(path):\n",
    "    \"\"\"Iterates through all .xml files (generated by labelImg) in a given directory and combines\n",
    "    them in a single Pandas dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    path : str\n",
    "        The path containing the .xml files\n",
    "    Returns\n",
    "    -------\n",
    "    Pandas DataFrame\n",
    "        The produced dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    xml_list = []\n",
    "    for xml_file in glob.glob(path + '/*.xml'):\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        filename = root.find('filename').text\n",
    "        width = int(root.find('size').find('width').text)\n",
    "        height = int(root.find('size').find('height').text)\n",
    "        for member in root.findall('object'):\n",
    "            bndbox = member.find('bndbox')\n",
    "            value = (filename,\n",
    "                     width,\n",
    "                     height,\n",
    "                     member.find('name').text,\n",
    "                     int(bndbox.find('xmin').text),\n",
    "                     int(bndbox.find('ymin').text),\n",
    "                     int(bndbox.find('xmax').text),\n",
    "                     int(bndbox.find('ymax').text),\n",
    "                     )\n",
    "            xml_list.append(value)\n",
    "    column_name = ['filename', 'width', 'height',\n",
    "                   'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
    "    return xml_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfbc4e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "fdfbc4e2",
    "outputId": "528bd747-3f4c-422d-a7ef-3c9450f8dc32"
   },
   "outputs": [],
   "source": [
    "# write your code here\n",
    "xml_df = ...\n",
    "xml_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0ee5e7",
   "metadata": {
    "id": "eb0ee5e7"
   },
   "source": [
    "The annotations contain the box coordinates in the Pascal VOC system (`[xmin, ymin, xmax, ymax]`), but they only have two main classes, cats and dogs, instead of the expected 37 classes found in the description. As it turns out, there are more files in the `annotations` folder, namely, `list.txt`, `trainval.txt`, and `test.txt`.\n",
    "\n",
    "If you're in Google Colab, the command below will list the files inside the `annotations` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162fd285",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "162fd285",
    "outputId": "003928df-9e5d-4a91-faaa-9ef54d6a5773"
   },
   "outputs": [],
   "source": [
    "# if you chose a different root folder, change it accordingly\n",
    "!ls -l ./pets/oxford-iiit-pet/annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea04722",
   "metadata": {
    "id": "aea04722"
   },
   "source": [
    "Let's take a look at the `list.txt` file.\n",
    "\n",
    "If you're in Google Colab, the command below will show you the first few lines of the `list.txt` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befa668e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "befa668e",
    "outputId": "f3d95633-3d81-49f9-fb18-5485dfc04bfd"
   },
   "outputs": [],
   "source": [
    "!head ./pets/oxford-iiit-pet/annotations/list.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48445f85",
   "metadata": {
    "id": "48445f85"
   },
   "source": [
    "It contains a list of all images in the dataset, organized in four columns separated by spaces: Image, CLASS-ID, SPECIES, BREED ID. As it turns out, the \"class\" from the XML file is actually the species. We're interested in the true class ids, from 1 to 37, as stated in the description.\n",
    "\n",
    "Now, let's take a look at the file corresponding to the data you loaded, the `trainval` split.\n",
    "\n",
    "If you're in Google Colab, the command below will show you the first few lines of the `trainval.txt` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12022f87",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12022f87",
    "outputId": "676e3949-220b-49e8-a338-68817938d9b0"
   },
   "outputs": [],
   "source": [
    "!head ./pets/oxford-iiit-pet/annotations/trainval.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc69f55",
   "metadata": {
    "id": "8fc69f55"
   },
   "source": [
    "It clearly follows the same structure as the previous file, but it does not contain any headers, and it lists only the images that belong to the original train and validation split.\n",
    "\n",
    "We can load it in Pandas for easier visualization (just run the code below as is to visualize the dataframe with the information from the `trainval.txt` file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a85a4a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "c7a85a4a",
    "outputId": "c4824eb0-4198-44ed-a163-864c69b36650"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "trainval_df = pd.read_csv('./pets/oxford-iiit-pet/annotations/trainval.txt', sep=' ', header=None, names=['filename', 'class_id', 'species', 'breed_id'])\n",
    "trainval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b24f47",
   "metadata": {
    "id": "b0b24f47"
   },
   "source": [
    "Each filename has its own corresponding class index (`class_id`), but the label itself, as the descriptive name corresponding to the category is only available as part of the filename itself. We can easily extract it, though. Just run the code below as is to create a new column (`category`) in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fade5fcd",
   "metadata": {
    "id": "fade5fcd"
   },
   "outputs": [],
   "source": [
    "trainval_df['category'] = trainval_df['filename'].apply(lambda v: ' '.join([w.capitalize()\n",
    "                                                                            for w in v.split('_')[:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb328f7a",
   "metadata": {
    "id": "cb328f7a"
   },
   "source": [
    "Moreover, there are 3,680 rows, one for each image, but there are 3,687 annotations retrieved from the XML files. Why? It is important to highlight that:\n",
    "- some images may have more than one annotation/box - you saw that already in the Penn-Fudan dataset\n",
    "- some images probably have no annotations/boxes (you'll see that soon)\n",
    "\n",
    "We'll use the same custom dataset class `ObjDetectionDataset` once again, since it is prepared to take a CSV file or Pandas dataframe containing the annotations (filename, labels, xmin, ymin, xmax, and ymax columns), but keep in mind that only the filenames in the file/dataframe are going to be considered by it.\n",
    "\n",
    "Therefore, we need to build an annotations file/dataframe that includes filenames that have no annotations as well. It is better to keep images without annotations as negative cases, so we merge both dataframes and make sure that:\n",
    "- every filename is kept, so there are still 3,680 unique filenames after merging\n",
    "- the resulting dataframe has, at least, the following columns: `filename`, `label`, `category`, `xmin`, `ymin`, `xmax`, and `ymax`\n",
    "\n",
    "Run the code below as is to build the corresponding dataframe of annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064de5ad",
   "metadata": {
    "id": "064de5ad"
   },
   "outputs": [],
   "source": [
    "trainval_df['filename'] = trainval_df['filename'].apply(lambda v: f'{v}.jpg')\n",
    "annotations_df = trainval_df.merge(xml_df, how='left', on='filename')\n",
    "\n",
    "colnames = ['filename', 'label', 'category', 'width', 'height', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "annotations_df = annotations_df.rename(columns={'class_id': 'label'})[colnames]\n",
    "annotations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c28adb2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "52b0218f",
    "outputId": "677aaa7e-af8b-4bfb-ebda-4a24b18ac751"
   },
   "source": [
    "Besides, we'll use the resulting dataframe to build a `id2label` dictionary to map class id into the corresponding category. Run the code below as is to build the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1710e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db1710e5",
    "outputId": "a410f7ba-8a61-44c1-de74-c0d57efcf917"
   },
   "outputs": [],
   "source": [
    "id2label = dict(annotations_df[['label', 'category']].drop_duplicates().values)\n",
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7527fd4c",
   "metadata": {},
   "source": [
    "Let's run some assert commands to ensure everything is as expected. Run the code below as is. It shouldn't raise any errors nor produce any output. If an error is raised, you should double-check the code for loading the dataset and its annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7d39d2",
   "metadata": {
    "id": "fa7d39d2"
   },
   "outputs": [],
   "source": [
    "assert len(annotations_df['filename'].unique()) == 3680\n",
    "assert len(id2label.values()) == 37\n",
    "assert len(annotations_df) == 3681"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2e25b4",
   "metadata": {
    "id": "2f2e25b4"
   },
   "source": [
    "Shouldn'it be 3,687? Perhaps even more, since it should also include images without any annotations? It actually should, but some of the annotated images were excluded from the `trainval.txt` list of files for some unknown reason. In case you're curious, these are the images. Run the code below as is to visualize the extra annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e528296e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e528296e",
    "outputId": "89dbbc3a-1546-47c9-e176-bf79c216fb8c"
   },
   "outputs": [],
   "source": [
    "extra_annotations = set(xml_df['filename'].unique()).difference(set(annotations_df['filename'].unique()))\n",
    "extra_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d19d78",
   "metadata": {
    "id": "60d19d78"
   },
   "source": [
    "The whole point of this apparent detour from our main job here - fine-tuning an object detection model - is to illustrate the fact that every dataset has its issues, and you should always take your time to investigate how it's organized, if there are quality issues, and ensure it's in the right shape to be loaded into an instance of your dataset class.\n",
    "\n",
    "By the way, PyTorch's built-in dataset class for the Oxford-IIIT Pet Dataset handles this preprocssing (splitting filenames, building id2label dictionary, etc) in its [constructor method](https://pytorch.org/vision/main/_modules/torchvision/datasets/oxford_iiit_pet.html), in case you'd like to check it out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6986c8",
   "metadata": {
    "id": "9e6986c8"
   },
   "source": [
    "### 11.8.3 Train-Validation Split\n",
    "\n",
    "The original list of files does not give any indication regarding the split between training and validation sets, so you'll have to do it yourself.\n",
    "\n",
    "Our suggestion is to shuffle the filenames, and take a large part of them (e.g. 3,000) as training set, and the remaining files as validation set.\n",
    "\n",
    "Split the annotations dataframe in two, as the filenames in each dataframe determine which files are going be part of each dataset (assuming you're using our `ObjDetectionDataset`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a20f284",
   "metadata": {
    "id": "3a20f284"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(11)\n",
    "\n",
    "# Get all (unique) file names from the annotations dataframe\n",
    "# write your code here\n",
    "fnames = ...\n",
    "np.random.shuffle(fnames)\n",
    "\n",
    "# Create a boolean pandas series to determine if a given annotation belongs\n",
    "# to the training set\n",
    "# Tip: don't forget that images may have multiple annotations - make sure\n",
    "# two annotations of the same image don't end up in different sets\n",
    "# write your code here\n",
    "is_train = ...\n",
    "\n",
    "annotations = {}\n",
    "# Use the boolean series to slice the annotations dataframe\n",
    "# write your code here\n",
    "annotations['train'] = ...\n",
    "annotations['val'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5082d0b1",
   "metadata": {
    "id": "5082d0b1"
   },
   "source": [
    "### 11.8.4 Loading Model's Weights\n",
    "\n",
    "You're using a new backbone for your Faster R-CNN model, so you need to pick one that's different from ResNet50. You could, for example, choose a smaller model from the ResNet family, but it's likely more fun to choose a completely different model instead. We suggest you use MobileNet V2 as the new backbone.\n",
    "\n",
    "Once you choose the model, load its pretrained weights and the prescribed transformations that come with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2Botj0epelN4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Botj0epelN4",
    "outputId": "ab8dcd5f-43fe-4edc-d0f4-7670aae5a6dd"
   },
   "outputs": [],
   "source": [
    "from torchvision.models import get_weight\n",
    "\n",
    "# write your code here\n",
    "weights = ...\n",
    "transforms_fn = ...\n",
    "transforms_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15dda4e",
   "metadata": {
    "id": "c15dda4e"
   },
   "source": [
    "This is its `forward()` method (of MobileNet V2 transform, that is). Take a good look at the sequence of transformations it performs because, as you probably already guesses, this function is not compatible with V2 transforms, so you'll have to include them yourself - if needed - in your data augmentation pipeline (the next section).\n",
    "\n",
    "```python\n",
    "def forward(self, img: Tensor) -> Tensor:\n",
    "    img = F.resize(img, self.resize_size, interpolation=self.interpolation, antialias=self.antialias)\n",
    "    img = F.center_crop(img, self.crop_size)\n",
    "    if not isinstance(img, Tensor):\n",
    "        img = F.pil_to_tensor(img)\n",
    "    img = F.convert_image_dtype(img, torch.float)\n",
    "    img = F.normalize(img, mean=self.mean, std=self.std)\n",
    "    return img\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425ab7a6",
   "metadata": {
    "id": "425ab7a6"
   },
   "source": [
    "### 11.8.5 Data Augmentation\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step3.png)\n",
    "\n",
    "It is time to write your own `get_transform()` function that takes one argument, namely, Ã¬f it is performing transformations on the training or the validation set:\n",
    "- if it is the validation set, it should stick to the basics (hint: check the prescribed transformations to assess these points)\n",
    "  - make sure the image is in the right size/shape for the backbone of your choice\n",
    "  - convert, if needed, PIL images to tensors\n",
    "  - normalize the values\n",
    "- if it is in the training set, it may perform data augmentation as well:\n",
    "  - choose one or more data augmenting transformations\n",
    "  - sanitize bounding boxes, just in case\n",
    "\n",
    "Pay special attention to the order in which transformations will happen, to make sure the transformed image at the end of the pipeline does indeed match the requirements of the backbone model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27ec8ca",
   "metadata": {
    "id": "b27ec8ca"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "from torchvision.transforms import v2 as transforms\n",
    "\n",
    "augmenting = [\n",
    "    # Choose one (or more) augmentation transform(s), such as RandomHorizontalFlip, for example\n",
    "    # write your code here\n",
    "    ...\n",
    "]\n",
    "\n",
    "basic = [\n",
    "    # Include required transformations here, such as transforming PIL images into tensors\n",
    "    # and normalizing pixel values\n",
    "    # write your code here\n",
    "    ...\n",
    "]\n",
    "\n",
    "def get_transform(train):\n",
    "    ops = [\n",
    "        # Include resizing transformations here, to make images the right size for the chosen model\n",
    "        # write your code here\n",
    "        ...\n",
    "    ]\n",
    "    # Only does augmenting in training mode\n",
    "    if train:\n",
    "        ops.extend(augmenting)\n",
    "    # Basic transforms: to tensor, sanitizing, and normalizing\n",
    "    ops.extend(basic)\n",
    "    return transforms.Compose(ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bfd44b",
   "metadata": {
    "id": "b5bfd44b"
   },
   "source": [
    "### 11.8.6 Datasets and DataLoaders\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step4.png)\n",
    "\n",
    "In Chapter 10, we built a dataset class that handles the nitty-gritty details of wrapping images, boxes, and masks, and applying transformations to both images and targets. Let's use the same class once again. Just run the code below as is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e65871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.datapoints import Image, BoundingBox, BoundingBoxFormat, Mask\n",
    "from torchvision.ops import masks_to_boxes, box_area\n",
    "from torchvision.datasets import VisionDataset\n",
    "\n",
    "class ObjDetectionDataset(VisionDataset):\n",
    "    def __init__(self, image_folder, annotations=None, mask_folder=None, transforms=None):\n",
    "        super().__init__(image_folder, transforms, None, None)\n",
    "        # folder where images are stored\n",
    "        self.image_folder = image_folder\n",
    "        # path to a CSV file or pandas dataframe with annotations\n",
    "        self.annotations = annotations\n",
    "        # folder where masks, if any, are stored\n",
    "        self.mask_folder = mask_folder\n",
    "        # transforms/augmentations to be applied to images\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # gets the list of all images sorted by name\n",
    "        self.images = list(sorted(os.listdir(image_folder)))\n",
    "\n",
    "        self.df_boxes = None\n",
    "        assert (annotations is not None) or (mask_folder is not None), \"At least one, annotations or masks, must be supplied\"\n",
    "\n",
    "        # if a CSV or dataframe was prodivded\n",
    "        if annotations is not None:\n",
    "            if isinstance(annotations, str):\n",
    "                self.df_boxes = pd.read_csv(annotations)\n",
    "            else:\n",
    "                self.df_boxes = annotations\n",
    "            # makes sure the annotations are in the XYXY format\n",
    "            assert len(set(self.df_boxes.columns).intersection({'filename', 'xmin', 'ymin', 'xmax', 'ymax'})) == 5, \"Missing columns in CSV\"\n",
    "            # only annotated images are considered - it overwrites the images attribute\n",
    "            self.images = self.df_boxes['filename'].unique().tolist()\n",
    "\n",
    "        self.masks = None\n",
    "        # if there are masks, makes sure each image has its own mask\n",
    "        if mask_folder is not None:\n",
    "            self.masks = list(sorted(os.listdir(mask_folder)))\n",
    "            assert len(self.masks) == len(self.images), \"Every image must have one, and only one, mask\"\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = os.path.join(self.image_folder, self.images[idx])\n",
    "        image_tensor = read_image(image_filename, mode=ImageReadMode.RGB)\n",
    "        # gets the last two dimensions, height and width\n",
    "        image_hw = image_tensor.shape[-2:]\n",
    "\n",
    "        labels = None\n",
    "        # If there are masks, we work with them\n",
    "        if self.masks is not None:\n",
    "            mask_filename = os.path.join(self.mask_folder, self.masks[idx])\n",
    "            merged_mask = read_image(mask_filename)\n",
    "            # checks how many instances are present in the mask\n",
    "            # assumes the first one, zero, is background only\n",
    "            instances = merged_mask.unique()[1:]\n",
    "\n",
    "            # splits the merged mask, so there's one mask for instance\n",
    "            masks = (merged_mask == instances.view(-1, 1, 1))\n",
    "            # converts masks into boxes\n",
    "            boxes = masks_to_boxes(masks)\n",
    "            # uses the datapoints namespace to wrap the masks\n",
    "            wrapped_masks = Mask(masks)\n",
    "        # No masks, so we fallback to a DF of annotated boxes\n",
    "        else:\n",
    "            # retrieves the annotations for the corresponding image\n",
    "            annots = self.df_boxes.query(f'filename == \"{self.images[idx]}\"')\n",
    "            # keeps only the coordinates\n",
    "            boxes = torch.as_tensor(annots.dropna()[['xmin', 'ymin', 'xmax', 'ymax']].values)\n",
    "            # if there are labels available as well, retrieves them\n",
    "            if 'label' in annots.columns:\n",
    "                labels = torch.as_tensor(annots.dropna()['label'].values)\n",
    "            wrapped_masks = None\n",
    "\n",
    "        # uses the datapoints namespace to wrap the boxes\n",
    "        wrapped_boxes = BoundingBox(boxes, format=BoundingBoxFormat.XYXY, spatial_size=image_hw)\n",
    "        num_objs = len(boxes)\n",
    "\n",
    "        if len(boxes):\n",
    "            if labels is None:\n",
    "                # if there are no labels, we assume every instance is of\n",
    "                # the same, and only, class\n",
    "                labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "            area = box_area(wrapped_boxes)\n",
    "        else:\n",
    "            # Only background, no boxes\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            area = torch.tensor([0.], dtype=torch.float32)\n",
    "\n",
    "        # creates a target dictionary with all elements\n",
    "        target = {\n",
    "            'boxes': wrapped_boxes,\n",
    "            'area': area,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([idx+1]),\n",
    "            'iscrowd': torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        }\n",
    "        # if there are masks, includes them\n",
    "        if wrapped_masks is not None:\n",
    "            target['masks'] = wrapped_masks\n",
    "\n",
    "        # uses the datapoints namespace to wrap the image\n",
    "        image = Image(image_tensor)\n",
    "\n",
    "        # if there are transformations/augmentations\n",
    "        # apply them to the image and target\n",
    "        if self.transforms is not None:\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e07ca71",
   "metadata": {},
   "source": [
    "Create two datasets, one for training, and one for validation, and assign the corresponding transformations to each one of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c7d7d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84c7d7d2",
    "outputId": "3747fac2-e727-475c-f530-89cd62e4c89a"
   },
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "\n",
    "# write your code here\n",
    "datasets['train'] = ...\n",
    "datasets['val'] = ...\n",
    "\n",
    "len(datasets['train']), len(datasets['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecac31d",
   "metadata": {
    "id": "5ecac31d"
   },
   "source": [
    "Next, create two data loaders, one for each dataset. You should shuffle the training set, but not the validation one. Also, keep batch size small (e.g. two) to avoid out-of-memory issues in the GPU.\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f19e95",
   "metadata": {
    "id": "c9f19e95"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloaders = {}\n",
    "\n",
    "# write your code here\n",
    "dataloaders['train'] = ...\n",
    "dataloaders['val'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2ce997",
   "metadata": {
    "id": "5f2ce997"
   },
   "source": [
    "Try fetching a mini-batch from your training set. Just run the code below as is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fd55e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78fd55e3",
    "outputId": "eb346acc-f168-4763-cfd6-495061744f82"
   },
   "outputs": [],
   "source": [
    "next(iter(dataloaders['train']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed526b2b",
   "metadata": {
    "id": "ed526b2b"
   },
   "source": [
    "Did you get an error? No? Consider yourself lucky! At some point, it will raise an error, whenever an image with either zero or more than one annotation is included in the mini-batch.\n",
    "\n",
    "The collate function is the function used by the data loader to patch together multiple data points into a mini-batch. If your dataset is nothing but tensors, that's trivial: it only has to stack them up. Stacking them up, though, assumes every data point has exactly the same shape for its features.\n",
    "\n",
    "In object detection models, though, this is not guaranteed to be the case: one image may have no boxes, another one may have three boxes, and yet another one may have only one. Those cannot be stacked together.\n",
    "\n",
    "The solution, fortunately, is pretty easy, and it looks like this:\n",
    "\n",
    "```python\n",
    "lambda batch: tuple(zip(*batch))\n",
    "```\n",
    "\n",
    "Throw the lambda function above as the `collate_fn` argument of your data loaders, and try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaa06ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8aaa06ba",
    "outputId": "6989dc53-710d-4f8d-8a17-c0ce7e23a1e8"
   },
   "outputs": [],
   "source": [
    "dataloaders = {}\n",
    "\n",
    "# write your code here\n",
    "dataloaders['train'] = ...\n",
    "dataloaders['val'] = ...\n",
    "\n",
    "next(iter(dataloaders['train']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
