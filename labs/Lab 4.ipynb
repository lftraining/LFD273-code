{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67609b90",
   "metadata": {},
   "source": [
    "# Lab Instructions\n",
    "\n",
    "In the lab, you're presented a task such as building a dataset, training a model, or writing a training loop, and we'll provide the code structured in such a way that you can fill in the blanks in the code using the knowledge you acquired in the chapters that precede the lab. You should be able to find appropriate snippets of code in the course content that work well in the lab with minor or no adjustments.\n",
    "\n",
    "The blanks in the code are indicated by ellipsis (`...`) and comments (`# write your code here`).\n",
    "\n",
    "In some cases, we'll provide you partial code to ensure the right variables are populated and any code that follows it runs accordingly.\n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "x = ...\n",
    "```\n",
    "\n",
    "The solution should be a single statement that replaces the ellipsis, such as:\n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "x = [0, 1, 2]\n",
    "```\n",
    "\n",
    "In some other cases, when there is no new variable being created, the blanks are shown like in the example below: \n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "...\n",
    "```\n",
    "\n",
    "Although we're showing you only a single ellipsis (`...`), you may have to write more than one line of code to complete the step, such as:\n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "for i, xi in enumerate(x):\n",
    "    x[i] = xi * 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4a6ad7",
   "metadata": {},
   "source": [
    "## Installation Notes\n",
    "\n",
    "To run this notebook on Google Colab, you will need to install the following librarie: portalocker.\n",
    "\n",
    "In Google Colab, you can run the following command to install this library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331fa91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install portalocker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19840775",
   "metadata": {
    "id": "b17bded5"
   },
   "source": [
    "## 8.5 Lab 4: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f95ef70",
   "metadata": {
    "id": "4d1bb429"
   },
   "source": [
    "In this lab, you'll fine-tune an encoder-based model to perform sentiment analysis on the Standford Sentiment Treebank (SST2) dataset. You'll load RoBERTa's sibling, XLM-RoBERTa, use its prescribed transformations to preprocess text in the SST2 dataset, and fine-tune (train) it for one epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f6e603",
   "metadata": {
    "id": "585fd02c"
   },
   "source": [
    "### 8.5.1 Model\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step1.png)\n",
    "\n",
    "You'll use Torchtext's `XLMR_BASE_ENCODER` in this lab. Create an instance of a classification head (`RobertaClassificationHead`) to perform binary classification (we have two classes, \"positive\" and \"negative\" sentiment), matching the input dimensions to the embeddings generated by the base model, and then load the model with the head attached to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2830c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6453fb87",
    "outputId": "3ec2c9d6-fcb2-46d0-937e-f83bfc7d0468"
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "xlmr_base = torchtext.models.XLMR_BASE_ENCODER\n",
    "\n",
    "#write your code here\n",
    "classifier_head = ...\n",
    "\n",
    "# Tip: you can call a method from xlmr_base to load the model with the head\n",
    "# write your code here\n",
    "model = ...\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090cc054",
   "metadata": {
    "id": "52f10c74"
   },
   "source": [
    "### 8.5.2 Dataset\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step1.png)\n",
    "\n",
    "Now, you will load Torchtext's [\"Stanford Sentiment Treebank (SST2)\"](https://pytorch.org/text/stable/datasets.html#sst2) dataset. This dataset uses Torchdata's `DataPipe`s instead of traditional `Dataset`s. It is already split into `train`, `dev` (validation), and `test` sets. You only need to specify it in the `split` argument in the constructor of `SST2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea0c7b",
   "metadata": {
    "id": "65b37a60"
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import SST2\n",
    "\n",
    "datapipes = {}\n",
    "# write your code here\n",
    "datapipes['train'] = ...\n",
    "datapipes['val'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b03b23",
   "metadata": {
    "id": "f933d20a"
   },
   "source": [
    "Let's take a look at one data point from the SST2 dataset. Just run the code below as is to visualize the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b31d13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d15fa20",
    "outputId": "f2707daa-10dc-4bc8-b583-e9a43db5339d"
   },
   "outputs": [],
   "source": [
    "row = next(iter(datapipes['train']))\n",
    "text, label = row\n",
    "text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb7d7d",
   "metadata": {
    "id": "377fe053"
   },
   "source": [
    "Each data point is a tuple, containing a line of text, and the corresponding label - the sentiment (0 for negative, 1 for positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e959ea",
   "metadata": {
    "id": "6eebde7e"
   },
   "source": [
    "### 8.5.3 Transforms\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step3.png)\n",
    "\n",
    "You already know the drill: you must preprocess the input (the text) using the prescribed transformation for the model you're using, so it gets tokenized, converted into token ids, and prependend/appended with the appropriate special tokens.\n",
    "\n",
    "Retrieve the transformation function/model from the XLM-RoBERTa model, and write a function that takes a tuple of `(text, label)` and returns another tuple of `(list of tokens ids, label)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874be416",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0c217192",
    "outputId": "94cda470-7ed9-4584-9703-9a52a723cc3f"
   },
   "outputs": [],
   "source": [
    "# write your code here\n",
    "transform_fn = ...\n",
    "transform_fn(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b3a214",
   "metadata": {
    "id": "f18d864c"
   },
   "outputs": [],
   "source": [
    "def apply_transform(row):\n",
    "    text, label = row\n",
    "    # Use the transform_fn you retrieved in the previous cell to\n",
    "    # preprocess the text\n",
    "    # write your code here\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a243f9a",
   "metadata": {
    "id": "dcd58738"
   },
   "source": [
    "Let's apply your function to our data point to see if it is working as expected (just run the code below as is to visualize the output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af81056",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ca73f45",
    "outputId": "0d56f4f0-5e41-4fd5-9318-feb2b015dba2"
   },
   "outputs": [],
   "source": [
    "apply_transform(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e04a4",
   "metadata": {
    "id": "32ec29d1"
   },
   "source": [
    "Did you notice the transformation is returning a regular Python list of token ids, not a PyTorch tensor? Remember, we cannot make a tensor out of lists of different lengths (see section 6.3.3). The solution? Padding the shorter sentences, so they all have the same length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9XiDVJfx8ALk",
   "metadata": {
    "id": "9XiDVJfx8ALk"
   },
   "source": [
    "But, how can we think of padding sentences if we don't have a mini-batch yet? It turns out, datapipes offer a `batch()` method that we can leverage to group data points together as mini-batches way before even thinking of creating a data loader.\n",
    "\n",
    "Let's try one out (just run the code below as is to visualize the output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wvImMYY_6YZ2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wvImMYY_6YZ2",
    "outputId": "c8d6c9fd-0f10-4791-933e-491d6aba29ff",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batched_datapipe = datapipes['train'].map(apply_transform).batch(4)\n",
    "batch_of_tuples = next(iter(batched_datapipe))\n",
    "batch_of_tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mvQ-lGOo8jty",
   "metadata": {
    "id": "mvQ-lGOo8jty"
   },
   "source": [
    "The returned mini-batch is a list of four elements, each element being a tuple `(token ids, label)` returned as-is from the previous step of the datapipe.\n",
    "\n",
    "However, in order to pad the sequences, it would be much better to have the a list of list of token ids instead. In other words, we need to turn rows into columns, and there is a method for that as well: `rows2columnar()`. Even better, we can name the columns, and they will be returned as dictionary keys. Just run the code below as is to visualize the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "koauXJIm6uUt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "koauXJIm6uUt",
    "outputId": "98fc9262-0b33-470a-edf9-eaa192fb4002"
   },
   "outputs": [],
   "source": [
    "columnar_datapipe = batched_datapipe.rows2columnar(['token_ids', 'labels'])\n",
    "dict_of_batches = next(iter(columnar_datapipe))\n",
    "\n",
    "dict_of_batches['labels'], dict_of_batches['token_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4d4a43",
   "metadata": {
    "id": "7f4d4a43"
   },
   "source": [
    "Awesome, now we're ready for the next step!\n",
    "\n",
    "Write a function that takes a batch of (transformed) data points, pads the sequences (using `to_tensor` and the padding id provided above), and converts the labels into a tensor as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a11a8b3",
   "metadata": {
    "id": "38b91d89"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.functional import to_tensor\n",
    "\n",
    "padding_idx = transform_fn[1].vocab.lookup_indices(['<pad>'])[0]\n",
    "\n",
    "def tensor_batch(batch):\n",
    "    tokens = batch['token_ids']\n",
    "    labels = batch['labels']\n",
    "    # write your code here\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af2071e",
   "metadata": {
    "id": "9af2071e"
   },
   "source": [
    "Now, let's line up all these steps:\n",
    "- applying transformation\n",
    "- batching sequences\n",
    "- turning rows of tuples into columns\n",
    "- padding sequences\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step4.png)\n",
    "\n",
    "Just run the code below as is to apply all preprocessing steps to the datapipes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9847976a",
   "metadata": {
    "id": "50bded1a"
   },
   "outputs": [],
   "source": [
    "for k in datapipes.keys():\n",
    "    datapipes[k] = datapipes[k].map(apply_transform)\n",
    "    datapipes[k] = datapipes[k].batch(16)\n",
    "    datapipes[k] = datapipes[k].rows2columnar(['token_ids', 'labels'])\n",
    "    datapipes[k] = datapipes[k].map(tensor_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59771ed8",
   "metadata": {
    "id": "b77e025b"
   },
   "source": [
    "If we fetch from our data pipe, it should return a tuple of two tensors, each tensor containing as many rows as the mini-batch size. Just run the code below as is to visualize the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a96ea3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e870aca2",
    "outputId": "4e957dc6-1964-4487-aa69-d2bd0eb94a66"
   },
   "outputs": [],
   "source": [
    "dp_out = next(iter(datapipes['train']))\n",
    "dp_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eb413e",
   "metadata": {
    "id": "0d570206"
   },
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step5.png)\n",
    "\n",
    "Now, create a data loader for each data pipe. Since the batches are already defined inside the data pipe, the batch size should be `None`. It is still OK to shuffle the training set, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360a62eb",
   "metadata": {
    "id": "76c804f3"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloaders = {}\n",
    "# write your code here\n",
    "dataloaders['train'] = ...\n",
    "dataloaders['val'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b149960",
   "metadata": {
    "id": "923f3846"
   },
   "source": [
    "Now, let's fetch a mini-batch from our data loader (just run the code below as is to visualize the output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45c518b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05babc84",
    "outputId": "270b0792-7d7a-44b1-a491-b404e7c3f739"
   },
   "outputs": [],
   "source": [
    "dl_out = next(iter(dataloaders['train']))\n",
    "dl_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81963c60",
   "metadata": {
    "id": "d4eb7806"
   },
   "source": [
    "Do you see any difference between the two outputs, from the (batched) datapipe and the data loader? The former returns a tuple while the latter returns a list, but the contents are the same: a mini-batch of features and a mini-batch of labels. The length of the features may differ depending on how long the longest sequence in a given mini-batch is.\n",
    "\n",
    "Just run the two cells of code below as they are to visualize their outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed342ae2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "093b38c7",
    "outputId": "0d5cba8f-f09e-46b8-ec5c-83a40aca5a1d"
   },
   "outputs": [],
   "source": [
    "dp_out[0].shape, dl_out[0].shape # features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe523ba5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5e15e9f",
    "outputId": "3d2a2e36-9020-4f7e-e65a-aefc52354546"
   },
   "outputs": [],
   "source": [
    "dp_out[1].shape, dl_out[1].shape # labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb739fb",
   "metadata": {
    "id": "0374eb63"
   },
   "source": [
    "This means that it is possible to use data pipes directly in the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd178d3d",
   "metadata": {
    "id": "171714e6"
   },
   "source": [
    "### 8.5.4 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36895afc",
   "metadata": {
    "id": "260d405e"
   },
   "source": [
    "Now, it is time to write a training loop to fine-tune your XLM-RoBERTa model on the SST2 dataset. This is a large model, and the training set has over 60,000 data points, so you can train it over a single epoch, that is, looping over the mini-batches from the datapipe (or data loader) only once. For the sake of speed, keep the evalution for the end only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08043f4",
   "metadata": {},
   "source": [
    "#### 8.5.4.1 Loss Function\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step2.png)\n",
    "\n",
    "Sentiment analysis is a classification task, so we need to use the appropriate loss function for the task. Even though it is a binary classification, RoBERTa's classification head is actually producing two logits instead of one, so you have to use `CrossEntropyLoss` (which can handle two or more logits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafd3679",
   "metadata": {
    "id": "330c949d"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_fn = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2bc46a",
   "metadata": {},
   "source": [
    "#### 8.5.4.2 Optimizer\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b2a9b5",
   "metadata": {},
   "source": [
    "Although `Adam` is the optimizer of choice, we suggest you try out `AdamW`, a modified version that is also commonly used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbfdcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# suggested learning rate\n",
    "lr = 1e-5\n",
    "\n",
    "optimizer = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3c47f2",
   "metadata": {},
   "source": [
    "#### 8.4.4.2 Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8040319f",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step4.png)\n",
    "\n",
    "So far, we haven't logged or inspected our losses in real-time. Why bother, if it takes only a minute to train the model? This time is different, though: fine-tuning RoBERTa on more than 67,000 data points, even for a single epoch, will take about 15 min or so in Google Colab. So, let's use TensorBoard to see how our loss is doing as training progresses.\n",
    "\n",
    "First, we need to load it using the corresponding Jupyter magic (just run the code below as is to load TensorBoard):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78659bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9a5a10",
   "metadata": {},
   "source": [
    "Next, we need to create an instance of the `SummaryWriter` to be able to send loss values to TensorBoard. Just run the code below as is to create it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72030ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/roberta')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12bc204",
   "metadata": {},
   "source": [
    "Now, it's your turn to write the missing parts of the training loop below. We have already taken care of the sending the losses to TensorBoard for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeb837e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7b75295",
    "outputId": "beb68d2c-62a9-4523-8e05-b6aeb41ee3e9"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "batch_losses = []\n",
    "\n",
    "## Training\n",
    "for i, (batch_features, batch_targets) in tqdm(enumerate(datapipes['train'])):\n",
    "    # Set the model's mode\n",
    "    # write your code here\n",
    "    ...\n",
    "    \n",
    "    # Send batch features and targets to the device\n",
    "    # write your code here\n",
    "    ...\n",
    "    \n",
    "    # Step 1 - forward pass\n",
    "    # write your code here\n",
    "    predictions = ...\n",
    "\n",
    "    # Step 2 - computing the loss\n",
    "    # write your code here\n",
    "    loss = ...\n",
    "\n",
    "    # Step 3 - computing the gradients\n",
    "    # Tip: it requires a single method call to backpropagate gradients\n",
    "    # write your code here\n",
    "    ...\n",
    "\n",
    "    batch_losses.append(loss.item())\n",
    "    \n",
    "    writer.add_scalars(main_tag='loss',\n",
    "                       tag_scalar_dict={'training': loss.item()},\n",
    "                       global_step=i)    \n",
    "\n",
    "    # Step 4 - updating parameters and zeroing gradients\n",
    "    # Tip: it takes two calls to optimizer's methods\n",
    "    # write your code here\n",
    "    ...\n",
    "\n",
    "\n",
    "writer.close()\n",
    "\n",
    "## Validation   \n",
    "with torch.inference_mode():\n",
    "    val_losses = []\n",
    "\n",
    "    for i, (val_features, val_targets) in enumerate(dataloaders['val']):\n",
    "        # Set the model's mode\n",
    "        # write your code here\n",
    "        ...\n",
    "\n",
    "        # Send batch features and targets to the device\n",
    "        # write your code here\n",
    "        ...\n",
    "\n",
    "        # Step 1 - forward pass\n",
    "        # write your code here\n",
    "        predictions = ...\n",
    "\n",
    "        # Step 2 - computing the loss\n",
    "        # write your code here\n",
    "        loss = ...\n",
    "        \n",
    "        val_losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad616e85",
   "metadata": {},
   "source": [
    "By the end of it, your losses on TensorBoard should look more or less like this (if you drag the slider on the right to the maximum level of smoothing):\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch6/tensorboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad40d3b",
   "metadata": {
    "id": "724660c3"
   },
   "source": [
    "### 8.5.5 Inference\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step5.png)\n",
    "\n",
    "Write a function that takes some text (a sequence of words), a model, its prescribed transformations, and a list of target categories for the classification, and returns the most likely category and the corresponding probability.\n",
    "\n",
    "Since you're handling a single sequence, there's no need for any padding, but you still need to provide a tensor containing a mini-batch (of one) as input to the model.\n",
    "\n",
    "The model returns two logits, one for each class, so you must use the softmax function to convert them into probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a3fe10",
   "metadata": {
    "id": "a9a8d274"
   },
   "outputs": [],
   "source": [
    "def predict(sequence, model, transforms_fn, categories):        \n",
    "    # Build a tensor of token ids out of the input sequence\n",
    "    # write your code here\n",
    "    ...\n",
    "\n",
    "    # Set the model to the appropriate mode\n",
    "    # write your code here\n",
    "    ...\n",
    "\n",
    "    device = next(iter(model.parameters())).device\n",
    "    \n",
    "    # Use the model to make predictions/logits\n",
    "    # Tip: Don't forget to send the input to the same device as the model\n",
    "    # Tip: Don't forget models take mini-batches as inputs, not single data points\n",
    "    # write your code here\n",
    "    pred = ...\n",
    "    \n",
    "    # Compute the probabilities corresponding to the logits\n",
    "    # and return the top value and index\n",
    "    # write your code here\n",
    "    probabilities = ...\n",
    "    values, indices = ...\n",
    "    \n",
    "    return [{'label': categories[i], 'value': v.item()} for i, v in zip(indices, values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af09745c",
   "metadata": {
    "id": "b587f62f"
   },
   "source": [
    "Now, try out your prediction function and fine-tuned model (just run the code cells below as they are to visualize their outputs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adef2d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fe87a17e",
    "outputId": "b451dba7-883f-413b-ac19-6e6a63ca6bbf"
   },
   "outputs": [],
   "source": [
    "categories = ['negative', 'positive']\n",
    "text = \"I am really liking this course\"\n",
    "predict(text, model, xlmr_base.transform(), categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce485dc1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b50f0c7",
    "outputId": "f9b1f8d0-602a-4339-ce21-a2b3e961a3e4"
   },
   "outputs": [],
   "source": [
    "text = \"This course is too complicated!\"\n",
    "predict(text, model, xlmr_base.transform(), categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b33b83f",
   "metadata": {
    "id": "578f344d"
   },
   "source": [
    "That's cool, but what if we could perform sentiment analysis out-of-the-box? That's what we'll do in the second part of Chapter 6."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
