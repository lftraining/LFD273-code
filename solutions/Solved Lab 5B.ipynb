{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67609b90",
   "metadata": {},
   "source": [
    "# Lab Instructions\n",
    "\n",
    "In the lab, you're presented a task such as building a dataset, training a model, or writing a training loop, and we'll provide the code structured in such a way that you can fill in the blanks in the code using the knowledge you acquired in the chapters that precede the lab. You should be able to find appropriate snippets of code in the course content that work well in the lab with minor or no adjustments.\n",
    "\n",
    "The blanks in the code are indicated by ellipsis (`...`) and comments (`# write your code here`).\n",
    "\n",
    "In some cases, we'll provide you partial code to ensure the right variables are populated and any code that follows it runs accordingly.\n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "x = ...\n",
    "```\n",
    "\n",
    "The solution should be a single statement that replaces the ellipsis, such as:\n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "x = [0, 1, 2]\n",
    "```\n",
    "\n",
    "In some other cases, when there is no new variable being created, the blanks are shown like in the example below: \n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "...\n",
    "```\n",
    "\n",
    "Although we're showing you only a single ellipsis (`...`), you may have to write more than one line of code to complete the step, such as:\n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "for i, xi in enumerate(x):\n",
    "    x[i] = xi * 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee17cb89",
   "metadata": {
    "id": "ee17cb89"
   },
   "source": [
    "### Installation Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ca2a8e",
   "metadata": {
    "id": "a4ca2a8e"
   },
   "source": [
    "The `xml_to_csv()` function can be easily imported from a set of helper functions we're making available for your convenience. You can download it from the following link:\n",
    "\n",
    "```\n",
    "https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/helper_functions.py\n",
    "```\n",
    "\n",
    "In Google Colab, you can run the following command to download the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c23d850",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9c23d850",
    "outputId": "e680b14a-cfc8-4612-ccd7-a0e9bb03ec07"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/helper_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1239cd0",
   "metadata": {
    "id": "e1239cd0"
   },
   "source": [
    "Once the file is downloaded, you only need to import the required helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59698994",
   "metadata": {
    "id": "59698994"
   },
   "outputs": [],
   "source": [
    "from helper_functions import xml_to_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbc6940",
   "metadata": {
    "id": "888c5e9f"
   },
   "source": [
    "## 13.4 Lab 5B: Fine-Tuning Object Detection Models\n",
    "\n",
    "In this lab, you'll build a dataset, including data augmentation, and fine-tune a custom object detection model by replacing its standard backbone with a different computer vision model. In the end, you'll evaluate the model using metrics from the COCO challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efd6fe1",
   "metadata": {},
   "source": [
    "### 13.4.1 Recap\n",
    "\n",
    "Let's recap what we did in the last lab to properly load and preprocess our dataset, so we can use it to train a non-linear regression in PyTorch. You may run all the cells in this section as they are.\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step1.png)\n",
    "\n",
    "First, we loaded the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdab971",
   "metadata": {
    "id": "432568a3"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import OxfordIIITPet\n",
    "\n",
    "root_folder = './pets'\n",
    "pets = OxfordIIITPet(root='./pets', split='trainval', target_types=['category', 'segmentation'], download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e1091f",
   "metadata": {},
   "source": [
    "Then, we loaded its annotations into a dataframe, and built a dictionary of categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c264077a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "c7a85a4a",
    "outputId": "c4824eb0-4198-44ed-a163-864c69b36650"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "xml_df = xml_to_csv(f'{root_folder}/oxford-iiit-pet/annotations/xmls')\n",
    "\n",
    "trainval_df = pd.read_csv('./pets/oxford-iiit-pet/annotations/trainval.txt', sep=' ', header=None, names=['filename', 'class_id', 'species', 'breed_id'])\n",
    "trainval_df['category'] = trainval_df['filename'].apply(lambda v: ' '.join([w.capitalize()\n",
    "                                                                            for w in v.split('_')[:-1]]))\n",
    "trainval_df['filename'] = trainval_df['filename'].apply(lambda v: f'{v}.jpg')\n",
    "annotations_df = trainval_df.merge(xml_df, how='left', on='filename')\n",
    "\n",
    "colnames = ['filename', 'label', 'category', 'width', 'height', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "annotations_df = annotations_df.rename(columns={'class_id': 'label'})[colnames]\n",
    "\n",
    "id2label = dict(annotations_df[['label', 'category']].drop_duplicates().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577e40d6",
   "metadata": {},
   "source": [
    "Next, we used the annotations to split the dataset into training and validation sets using the filenames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2386c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(11)\n",
    "\n",
    "fnames = sorted(annotations_df['filename'].unique())\n",
    "np.random.shuffle(fnames)\n",
    "\n",
    "is_train = annotations_df['filename'].isin(fnames[:3000])\n",
    "\n",
    "annotations = {}\n",
    "annotations['train'] = annotations_df[is_train]\n",
    "annotations['val'] = annotations_df[~is_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8218006b",
   "metadata": {},
   "source": [
    "Preprocessing images and performing data augmentation is a big part of training a model, so we created a function that applies the transformations to an image, depending on which dataset it belongs to:\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0907bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "from torchvision.transforms import v2 as transforms\n",
    "\n",
    "augmenting = [\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "]\n",
    "\n",
    "basic = [\n",
    "    # transforms.ToTensor() was deprecated so we\n",
    "    # replace it by the two transforms below\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    # it is a no-op in this flow, but it may\n",
    "    # be necessary if we use different augmentations\n",
    "    transforms.SanitizeBoundingBoxes(),\n",
    "    # last op from transforms_fn\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "]\n",
    "\n",
    "def get_transform(train):\n",
    "    # Starts by applying transformations to\n",
    "    # get images to the right size\n",
    "    ops = [\n",
    "        # from transforms_fn\n",
    "        transforms.Resize(232, antialias=True),\n",
    "        transforms.CenterCrop(224)\n",
    "    ]\n",
    "    # Only does augmenting in training mode\n",
    "    if train:\n",
    "        ops.extend(augmenting)\n",
    "    # Basic transforms: to tensor, sanitizing, and normalizing\n",
    "    ops.extend(basic)\n",
    "    return transforms.Compose(ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb604b82",
   "metadata": {},
   "source": [
    "In the previous lab, we built a custom dataset that can handle the nitty-gritty details of organizing the images and their corresponding annotations, as well as applying transformations to images and targets:\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1d25fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.tv_tensors import Image, BoundingBoxes, BoundingBoxFormat, Mask\n",
    "from torchvision.ops import masks_to_boxes, box_area\n",
    "from torchvision.datasets import VisionDataset\n",
    "\n",
    "class ObjDetectionDataset(VisionDataset):\n",
    "    def __init__(self, image_folder, annotations=None, mask_folder=None, transforms=None):\n",
    "        super().__init__(image_folder, transforms, None, None)\n",
    "        # folder where images are stored\n",
    "        self.image_folder = image_folder\n",
    "        # path to a CSV file or pandas dataframe with annotations\n",
    "        self.annotations = annotations\n",
    "        # folder where masks, if any, are stored\n",
    "        self.mask_folder = mask_folder\n",
    "        # transforms/augmentations to be applied to images\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # gets the list of all images sorted by name\n",
    "        self.images = list(sorted(os.listdir(image_folder)))\n",
    "\n",
    "        self.df_boxes = None\n",
    "        assert (annotations is not None) or (mask_folder is not None), \"At least one, annotations or masks, must be supplied\"\n",
    "\n",
    "        # if a CSV or dataframe was prodivded\n",
    "        if annotations is not None:\n",
    "            if isinstance(annotations, str):\n",
    "                self.df_boxes = pd.read_csv(annotations)\n",
    "            else:\n",
    "                self.df_boxes = annotations\n",
    "            # makes sure the annotations are in the XYXY format\n",
    "            assert len(set(self.df_boxes.columns).intersection({'filename', 'xmin', 'ymin', 'xmax', 'ymax'})) == 5, \"Missing columns in CSV\"\n",
    "            # only annotated images are considered - it overwrites the images attribute\n",
    "            self.images = self.df_boxes['filename'].unique().tolist()\n",
    "\n",
    "        self.masks = None\n",
    "        # if there are masks, makes sure each image has its own mask\n",
    "        if mask_folder is not None:\n",
    "            self.masks = list(sorted(os.listdir(mask_folder)))\n",
    "            assert len(self.masks) == len(self.images), \"Every image must have one, and only one, mask\"\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = os.path.join(self.image_folder, self.images[idx])\n",
    "        image_tensor = read_image(image_filename, mode=ImageReadMode.RGB)\n",
    "        # gets the last two dimensions, height and width\n",
    "        image_hw = image_tensor.shape[-2:]\n",
    "\n",
    "        labels = None\n",
    "        # If there are masks, we work with them\n",
    "        if self.masks is not None:\n",
    "            mask_filename = os.path.join(self.mask_folder, self.masks[idx])\n",
    "            merged_mask = read_image(mask_filename)\n",
    "            # checks how many instances are present in the mask\n",
    "            # assumes the first one, zero, is background only\n",
    "            instances = merged_mask.unique()[1:]\n",
    "\n",
    "            # splits the merged mask, so there's one mask for instance\n",
    "            masks = (merged_mask == instances.view(-1, 1, 1))\n",
    "            # converts masks into boxes\n",
    "            boxes = masks_to_boxes(masks)\n",
    "            # uses the datapoints namespace to wrap the masks\n",
    "            wrapped_masks = Mask(masks)\n",
    "        # No masks, so we fallback to a DF of annotated boxes\n",
    "        else:\n",
    "            # retrieves the annotations for the corresponding image\n",
    "            annots = self.df_boxes.query(f'filename == \"{self.images[idx]}\"')\n",
    "            # keeps only the coordinates\n",
    "            boxes = torch.as_tensor(annots.dropna()[['xmin', 'ymin', 'xmax', 'ymax']].values)\n",
    "            # if there are labels available as well, retrieves them\n",
    "            if 'label' in annots.columns:\n",
    "                labels = torch.as_tensor(annots.dropna()['label'].values)\n",
    "            wrapped_masks = None\n",
    "\n",
    "        # uses the datapoints namespace to wrap the boxes\n",
    "        wrapped_boxes = BoundingBoxes(boxes, format=BoundingBoxFormat.XYXY, canvas_size=image_hw)\n",
    "        num_objs = len(boxes)\n",
    "\n",
    "        if len(boxes):\n",
    "            if labels is None:\n",
    "                # if there are no labels, we assume every instance is of\n",
    "                # the same, and only, class\n",
    "                labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "            area = box_area(wrapped_boxes)\n",
    "        else:\n",
    "            # Only background, no boxes\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            area = torch.tensor([0.], dtype=torch.float32)\n",
    "\n",
    "        # creates a target dictionary with all elements\n",
    "        target = {\n",
    "            'boxes': wrapped_boxes,\n",
    "            'area': area,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([idx+1]),\n",
    "            'iscrowd': torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        }\n",
    "        # if there are masks, includes them\n",
    "        if wrapped_masks is not None:\n",
    "            target['masks'] = wrapped_masks\n",
    "\n",
    "        # uses the datapoints namespace to wrap the image\n",
    "        image = Image(image_tensor)\n",
    "\n",
    "        # if there are transformations/augmentations\n",
    "        # apply them to the image and target\n",
    "        if self.transforms is not None:\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd423c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "datasets['train'] = ObjDetectionDataset(image_folder='./pets/oxford-iiit-pet/images',\n",
    "                                        annotations=annotations['train'],\n",
    "                                        transforms=get_transform(True))\n",
    "datasets['val'] = ObjDetectionDataset(image_folder='./pets/oxford-iiit-pet/images',\n",
    "                                      annotations=annotations['val'],\n",
    "                                      transforms=get_transform(False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c518c313",
   "metadata": {},
   "source": [
    "Once the datasets are ready, we created data loaders so we can load mini-batches of data, one at a time:\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a98ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloaders = {}\n",
    "dataloaders['train'] = DataLoader(datasets['train'], batch_size=2, shuffle=True, collate_fn=lambda batch: tuple(zip(*batch)))\n",
    "dataloaders['val'] = DataLoader(datasets['val'], batch_size=2, shuffle=False, collate_fn=lambda batch: tuple(zip(*batch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71690ee",
   "metadata": {
    "id": "b71690ee"
   },
   "source": [
    "### 13.4.2 Model\n",
    "\n",
    "Now, the fun part begins: replacing the backbone of a pretrained object detection model!\n",
    "\n",
    "You will have to create a brand new instance of the `FasterRCNN` class using the required arguments to make your model work:\n",
    "   - `backbone`: your feature extractor\n",
    "   - `rpn_anchor_generator`: the new anchor generator\n",
    "   - `box_roi_pool`: the new ROI pooler\n",
    "   - `num_classes`: the number of classes for your task\n",
    "\n",
    "You already know the number of classes - but don't forget another one for the negative case, that is, whenever there's no object in the image. This class (for the background, if you will) is usually assigned the zero index (and that's why the class indices from the dataset start at one).\n",
    "\n",
    "You also have the weights for the backbone model too, but you need to create a model that returns its features only (the \"headless\" model, as seen in Chapter 2). The model must return either a feature map dictionary (if you're extracing features from multiple layers of your backbone) or a single tensor (if you're extracting a single set of features). Also, keep in mind that:\n",
    "   - some models (like [MobileNet V2](https://pytorch.org/hub/pytorch_vision_mobilenet_v2/), our suggested choice of new bacbone) can have its features extracted easily accesing a single attribute (`features` in the case of MobileNet)\n",
    "   - for more complex models, you can use `create_feature_extractor()` or `IntermediateLayerGetter` to build your backbone\n",
    "\n",
    "Use the weights you already loaded to create an instance of your backbone model and use one of the alternatives above to get its features only returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a6e509",
   "metadata": {
    "id": "01a6e509"
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models import mobilenet_v2, get_weight\n",
    "\n",
    "num_classes = len(id2label) + 1\n",
    "\n",
    "weights = get_weight('MobileNet_V2_Weights.DEFAULT')\n",
    "mobilenet = mobilenet_v2(weights=weights)\n",
    "new_backbone = mobilenet.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f9dbd7",
   "metadata": {
    "id": "51f9dbd7"
   },
   "source": [
    "Double-check if your model is returning what you expect of it by feeding it a random tensor in the shape of a mini-batch (make sure the height and width of your random images match the expected input of your model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634835fd",
   "metadata": {
    "id": "634835fd"
   },
   "outputs": [],
   "source": [
    "dummy_x = torch.randn(2, 3, 224, 224)\n",
    "dummy_output = new_backbone(dummy_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed7437",
   "metadata": {
    "id": "81ed7437"
   },
   "source": [
    "You shouldn't get any errors, and your dummy output must be either a single tensor, or a feature map dictionary. Check the shape of each returned tensor (one or more), and make sure they all have the same number of output channels. This is required by the Faster R-CNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b91bce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71b91bce",
    "outputId": "2e799c93-831a-476d-d0ed-1fc5c6697490"
   },
   "outputs": [],
   "source": [
    "out_channels = dummy_output.shape[1]\n",
    "out_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7622af",
   "metadata": {
    "id": "fd7622af"
   },
   "source": [
    "Assign the number of output channels to the instance of your backbone as an `out_channels` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb119b",
   "metadata": {
    "id": "77eb119b"
   },
   "outputs": [],
   "source": [
    "# write your code here\n",
    "new_backbone.out_channels = 1280"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b384ea03",
   "metadata": {
    "id": "b384ea03"
   },
   "source": [
    "Create an instance of the `AnchorGenerator` class, and make sure each argument - `sizes` and `aspect_ratios` is a tuple containing as many elements as the number of feature maps returned by your backbone.\n",
    "\n",
    "Each element is a tuple itself, and may have as many elements as you wish. For more details, refer to the \"Region Proposal Network\" subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20171de",
   "metadata": {
    "id": "a20171de"
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "sizes = ((32, 64, 128, 256, 512),)\n",
    "aspect_ratios = ((0.5, 1.0, 2.0),)\n",
    "\n",
    "# write your code here\n",
    "anchor_generator = AnchorGenerator(sizes=sizes, aspect_ratios=aspect_ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33b0591",
   "metadata": {
    "id": "c33b0591"
   },
   "source": [
    "Create an instance of the `MultiScaleRoIAlign` class, and make sure it points to at least one valid feature map as returned by our backbone model. For more details, refer to the \"Regions of Interest\" subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c932be4",
   "metadata": {
    "id": "7c932be4"
   },
   "outputs": [],
   "source": [
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "\n",
    "output_size = 7\n",
    "sampling_ratio = 2\n",
    "\n",
    "# Tip: simpler models don't return dictionaries, but feature maps are guaranteed to be a dictionary\n",
    "# containing, at least, a \"0\" key\n",
    "# write your code here\n",
    "roi_pooler = MultiScaleRoIAlign(featmap_names=['0'], output_size=7, sampling_ratio=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf1004c",
   "metadata": {
    "id": "9bf1004c"
   },
   "source": [
    "Now, put everything together as your own Faster R-CNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76fa345",
   "metadata": {
    "id": "e76fa345"
   },
   "outputs": [],
   "source": [
    "# write your code here\n",
    "model = FasterRCNN(new_backbone,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=roi_pooler,\n",
    "                   num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4e0d86",
   "metadata": {
    "id": "3e4e0d86"
   },
   "source": [
    "There you go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8387e1",
   "metadata": {
    "id": "5f8387e1"
   },
   "source": [
    "#### 13.4.2.1 Double-Checking the Model\n",
    "\n",
    "To make sure your configuration is working fine, you can feed your new Faster R-CNN model a random tensor representing a dummy mini-batch once again. If you don't get any errors back, you're likely good to go!\n",
    "\n",
    "Don't forget to send each tensor in your mini-batch, individually, to the device. You cannot simply send them all at once as you used to do before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7b8072",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b7b8072",
    "outputId": "e26ef5d7-a223-4fcf-8257-6430c3c2a262"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "images, targets = next(iter(dataloaders['train']))\n",
    "\n",
    "# Send images and targets to device\n",
    "# write your code here\n",
    "images = list(image.to(device) for image in images)\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "# Make predictions using your model - you should get a dict of losses back\n",
    "# write your code here\n",
    "output = model(images, targets)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450b9b34",
   "metadata": {
    "id": "450b9b34"
   },
   "source": [
    "### 13.4.3 Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34ddcdd",
   "metadata": {
    "id": "e34ddcdd"
   },
   "source": [
    "It is time to write a real training loop now! You can use the dummy loop as a template and build on top of it, once you're happy with your schedulers.\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step3.png)\n",
    "\n",
    "Don't forget to send every tensor, individually, to the same device as the model. Also, keep in mind that the model returns a dictionary with many separate losses. It is your job to sum them all up to compute gradients based on the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da8ccad",
   "metadata": {
    "id": "7da8ccad"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, LinearLR\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Decreases the learning rate by 10x every 3 epochs\n",
    "# write your code here\n",
    "lr_scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Warms-up the learning rate from zero to 0.005 over one epoch\n",
    "warmup_factor = 1.0 / 1000\n",
    "warmup_iters = min(1000, len(dataloaders['train']) - 1)\n",
    "\n",
    "# write your code here\n",
    "lr_scheduler2 = LinearLR(optimizer, start_factor=warmup_factor, total_iters=warmup_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee0835",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990a9910",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "990a9910",
    "outputId": "202095d9-776a-4a83-e220-c33f6f0b6e09"
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, targets) in enumerate(dataloaders['train']):\n",
    "        # Send images and targets to device\n",
    "        # write your code here\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Set your model's mode\n",
    "        # write your code here\n",
    "        model.train()\n",
    "\n",
    "        # Call the model to get a loss dict back\n",
    "        # write your code here\n",
    "        loss_dict = model(images, targets)\n",
    "        \n",
    "        if not (i % 50):\n",
    "            print([(k, v.item()) for k, v in loss_dict.items()])\n",
    "\n",
    "        # You have many losses in the dict, but you can only\n",
    "        # call backward one a single value, so you must\n",
    "        # add them up\n",
    "        # write your code here\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch == 0:\n",
    "            lr_scheduler2.step()\n",
    "\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28134429",
   "metadata": {
    "id": "28134429"
   },
   "source": [
    "Training this model takes quite a while...\n",
    "\n",
    "Once it's finished training, you can save it to disk for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81468794",
   "metadata": {
    "id": "81468794"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'mobilenet_v2_pets.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
