{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67609b90",
   "metadata": {},
   "source": [
    "# Lab Instructions\n",
    "\n",
    "In the lab, you're presented a task such as building a dataset, training a model, or writing a training loop, and we'll provide the code structured in such a way that you can fill in the blanks in the code using the knowledge you acquired in the chapters that precede the lab. You should be able to find appropriate snippets of code in the course content that work well in the lab with minor or no adjustments.\n",
    "\n",
    "The blanks in the code are indicated by ellipsis (`...`) and comments (`# write your code here`).\n",
    "\n",
    "In some cases, we'll provide you partial code to ensure the right variables are populated and any code that follows it runs accordingly.\n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "x = ...\n",
    "```\n",
    "\n",
    "The solution should be a single statement that replaces the ellipsis, such as:\n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "x = [0, 1, 2]\n",
    "```\n",
    "\n",
    "In some other cases, when there is no new variable being created, the blanks are shown like in the example below: \n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "...\n",
    "```\n",
    "\n",
    "Although we're showing you only a single ellipsis (`...`), you may have to write more than one line of code to complete the step, such as:\n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "for i, xi in enumerate(x):\n",
    "    x[i] = xi * 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4a6ad7",
   "metadata": {},
   "source": [
    "## Installation Notes\n",
    "\n",
    "To run this notebook on Google Colab, you will need to install the following libraries: transformers, evaluate, and portalocker.\n",
    "\n",
    "In Google Colab, you can run the following command to install these libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c02fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers evaluate portalocker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3771488d",
   "metadata": {
    "id": "c1f1a96f"
   },
   "source": [
    "## 15.10 Lab 6: Text Classification using Embeddings\n",
    "\n",
    "It is time to get our hands dirty! Let's use GloVe pretrained word embeddings as features for a multi-class linear classification model. It works like a linear regression model, but it produces four logits as output (one for each class in the AG News Dataset), and we'll use the softmax function to convert the logits into probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbd7042",
   "metadata": {},
   "source": [
    "### 15.10.1 Recap\n",
    "\n",
    "In the last chapter, we created \"raw\" data pipes that load the CSV files from the AG News Dataset, clean them up of special characters and HTML tags, and discard the title information, returning only labels and (cleaned) descriptions. Let's quickly retrace our steps here to prepare the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafad307",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step1.png)\n",
    "\n",
    "First, we need to download the dataset. You can dowload the files from the following links:\n",
    "- `https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv`\n",
    "- `https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv`\n",
    "- `https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/classes.txt`\n",
    "\n",
    "Alternatively, you can download all files as a single compressed file instead:\n",
    "\n",
    "```\n",
    "https://github.com/dvgodoy/assets/raw/main/PyTorchInPractice/data/AGNews/agnews.zip\n",
    "```\n",
    "\n",
    "If you're running Google Colab, you can download the files using the commands below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11466c94",
   "metadata": {
    "id": "11466c94"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
    "!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv\n",
    "!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/classes.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400ec92c",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step2.png)\n",
    "\n",
    "Next, let's do some data cleaning, getting rid of a few HTML tags, replacing some special characters, etc. Here is a non-exhaustive list of characters and tags for replacement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8cbebf",
   "metadata": {
    "id": "7c8cbebf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "chr_codes = np.array([\n",
    "     36,   151,    38,  8220,   147,   148,   146,   225,   133,    39,  8221,  8212,   232,   149,   145,   233,\n",
    "  64257,  8217,   163,   160,    91,    93,  8211,  8482,   234,    37,  8364,   153,   195,   169\n",
    "])\n",
    "chr_subst = {f' #{c};':chr(c) for c in chr_codes}\n",
    "chr_subst.update({' amp;': '&', ' quot;': \"'\", ' hellip;': '...', ' nbsp;': ' ', '&lt;': '', '&gt;': '',\n",
    "                  '&lt;em&gt;': '', '&lt;/em&gt;': '', '&lt;strong&gt;': '', '&lt;/strong&gt;': ''})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120e9170",
   "metadata": {},
   "source": [
    "And here are a couple of helper functions we used to perform the cleanup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2108e26",
   "metadata": {
    "id": "a2108e26"
   },
   "outputs": [],
   "source": [
    "def replace_chars(sent):\n",
    "    to_replace = [c for c in list(chr_subst.keys()) if c in sent]\n",
    "    for c in to_replace:\n",
    "        sent = sent.replace(c, chr_subst[c])\n",
    "    return sent\n",
    "\n",
    "def preproc_description(desc):\n",
    "    desc = desc.replace('\\\\', ' ').strip()\n",
    "    return replace_chars(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e466147b",
   "metadata": {},
   "source": [
    "Then, we used those functions to create a \"raw\" datapipe that loads the data from a CSV file, parses it, and applies the functions above to clean up the text. The function below also converts the label into a 0-based numeric value, and keeps only labels and clean up text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b126f81f",
   "metadata": {
    "id": "b126f81f"
   },
   "outputs": [],
   "source": [
    "from torchdata.datapipes.iter import FileLister\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def create_raw_datapipe(fname):\n",
    "    datapipe = FileLister(root='.')\n",
    "    datapipe = datapipe.filter(filter_fn=lambda v: v.endswith(fname))\n",
    "    datapipe = datapipe.open_files(mode='rt', encoding=\"utf-8\")\n",
    "    datapipe = datapipe.parse_csv(delimiter=\",\", skip_lines=0)\n",
    "    datapipe = datapipe.map(lambda row: (int(row[0])-1, preproc_description(row[2])))\n",
    "    return datapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083a93c9",
   "metadata": {},
   "source": [
    "In the previous chapter, we didn't actually train any models, so we didn't bother shuffling the training set. In this lab, however, we should:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63058350",
   "metadata": {
    "id": "63058350"
   },
   "outputs": [],
   "source": [
    "datapipes = {}\n",
    "datapipes['train'] = create_raw_datapipe('train.csv').shuffle(buffer_size=125000)\n",
    "datapipes['test'] = create_raw_datapipe('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a1dcae",
   "metadata": {},
   "source": [
    "### 15.10.2 Tokenizing and Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28da1e5",
   "metadata": {},
   "source": [
    "Let's plan ahead what needs to be done:\n",
    "- create data loaders, one for each data pipe\n",
    "- write a function that tokenizes the sentences in a given batch\n",
    "- retrieve the word embeddings for each and every token\n",
    "- create a linear model that takes the embedding vectors as features\n",
    "- create the appropriate loss function and optimizer\n",
    "- write a training loop\n",
    "\n",
    "Create two data loaders, one for each data pipe (training and validation/test). For now, use a small batch size, such as four, to be able to more easily peek at the values. Later on, you'll recreate the data loader with a more appropriate batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2232d593",
   "metadata": {
    "id": "e934f5e5"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloaders = {}\n",
    "# write your code here\n",
    "dataloaders['train'] = DataLoader(dataset=datapipes['train'], batch_size=4, shuffle=True)\n",
    "dataloaders['test'] = DataLoader(dataset=datapipes['test'], batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5480e8eb",
   "metadata": {},
   "source": [
    "Fetch one mini-batch of data to make sure it's working fine. Just run the code below as is to visualize the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc56995b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e35f7f98",
    "outputId": "2654e182-0026-4286-db16-e33360c546fa"
   },
   "outputs": [],
   "source": [
    "labels, sentences = next(iter(dataloaders['train']))\n",
    "labels, sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3efa46",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step3.png)\n",
    "\n",
    "Now, write a function that tokenizes a mini-batch of sentences. The function must take as arguments:\n",
    "- a tuple or list containing multiple sentences (as returned by the data loader)\n",
    "- an optional tokenizer: if the tokenizer isn't provided, it should fall back to the default `basic_english` tokenizer we have been using\n",
    "\n",
    "The function must return a list of lists of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1545d8fe",
   "metadata": {
    "id": "76582d02"
   },
   "outputs": [],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "def tokenize_batch(sentences, tokenizer=None):\n",
    "    # Create the basic tokenizer if one isn't provided\n",
    "    # write your code here\n",
    "    if tokenizer is None:\n",
    "        tokenizer = get_tokenizer('basic_english')\n",
    "    \n",
    "    # Tokenize sentences and returns the result\n",
    "    # write your code here\n",
    "    return [tokenizer(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a52bb41",
   "metadata": {},
   "source": [
    "Try your function out and assign its output to the `tokens` variable. Just run the code below as is to visualize the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925dfcda",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cede23a5",
    "outputId": "4ad42833-7d73-4152-e28e-998a788f61a2"
   },
   "outputs": [],
   "source": [
    "tokens = tokenize_batch(sentences)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc78810f",
   "metadata": {},
   "source": [
    "More likely than not, each sentence in a mini-batch has different number of tokens in it. How many tokens are there in each sentence? Just run the code below as is to see the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2290483",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8da1218",
    "outputId": "b5a01f17-d77c-4387-89a9-689569916055"
   },
   "outputs": [],
   "source": [
    "[len(s) for s in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783b90b2",
   "metadata": {},
   "source": [
    "Now, let's briefly discuss two different approaches to handling this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0366d82b",
   "metadata": {
    "id": "43280d03"
   },
   "source": [
    "#### 15.10.2.1 Alternative 1: Padding\n",
    "\n",
    "Did padding come to your mind? We have taken this approach time and again. However, we've always performed it on top of token indices, not tokens themselves: that's what we used `ToTensor()` for.\n",
    "\n",
    "Now, you'll write a function called `fixed_length()` that combines both truncating and padding operations at token (word) level. The function must take as arguments:\n",
    "- a list of lists of tokens (as returned by the `tokenize_batch()` function)\n",
    "- the maximum length of tokens, above which they are truncated\n",
    "- the string that represents the padding token (default `<pad>`)\n",
    "\n",
    "The function must truncate sequences of tokens that are too long and, afterward, pad the sequences so the shorter ones match the length of the longest.\n",
    "\n",
    "It must return a list of lists of tokens, every inner list having the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8eb9cc",
   "metadata": {
    "id": "eadf4b90"
   },
   "outputs": [],
   "source": [
    "def fixed_length(tokens_batch, max_len=128, pad_token='<pad>'):\n",
    "    # Truncate every sentence to max_len\n",
    "    # write your code here\n",
    "    truncated = [s[:max_len] for s in tokens_batch]\n",
    "    \n",
    "    # Check the actual maximum length of the (truncated) inputs\n",
    "    # write your code here\n",
    "    current_max = max([len(s) for s in truncated])\n",
    "    \n",
    "    # Appends as many padding tokens as necessary to make every\n",
    "    # sentence as long as the actual maximum length\n",
    "    # write your code here\n",
    "    padded = [s + [pad_token]*(current_max-len(s)) for s in truncated]\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4d2ebc",
   "metadata": {},
   "source": [
    "Double-check that every inner list has the same length, as expected. Just run the code below as is to visualize the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379b9c62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "d44fb5fc",
    "outputId": "6dd864ed-d386-475f-8656-98a1f301a9f3"
   },
   "outputs": [],
   "source": [
    "lengths = [len(s) for s in fixed_length(tokens)]\n",
    "lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c934e052",
   "metadata": {},
   "source": [
    "Same length everywhere? Great!\n",
    "\n",
    "Now, run the code below to load and uncompress GloVe vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ecee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "new_locations = {key: os.path.join('https://huggingface.co/stanfordnlp/glove/resolve/main',\n",
    "                                   os.path.split(GloVe.url[key])[-1]) for key in GloVe.url.keys()}\n",
    "GloVe.url = new_locations\n",
    "\n",
    "vec = GloVe(name='6B', dim=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e64f67",
   "metadata": {},
   "source": [
    "Next, write a function that takes as arguments:\n",
    "- a list of lists of tokens\n",
    "- an instance of `Vectors` (such as our own GloVe)\n",
    "\n",
    "And retrieves the corresponding embeddings as a tensor in the shape (N, L, D) where:\n",
    "- N is the number of data points in a mini-batch\n",
    "- L is the number of tokens in each sequence (they all have the same length now)\n",
    "- D is the number of dimensions in each embedding vector (50 in our instance of GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "726ccf71",
   "metadata": {
    "id": "dff3fecf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_embeddings(tokens, vec):\n",
    "    # Pad all lists so they have matching lengths\n",
    "    # write your code here\n",
    "    padded = fixed_length(tokens)\n",
    "    \n",
    "    # Retrieve embeddings from the Vector object using `get_vecs_by_tokens`\n",
    "    # Make sure to get the shapes right, and concatenate the tensors so\n",
    "    # the resulting shape is N, L, D\n",
    "    # write your code here\n",
    "    embeddings = torch.cat([vec.get_vecs_by_tokens(s).unsqueeze(0) for s in padded], dim=0)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afc866d",
   "metadata": {},
   "source": [
    "Just run the code below as is to inspect the shape of the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e05d46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88af0fef",
    "outputId": "aab558ba-29e5-42e1-a080-461fcd1d0b90"
   },
   "outputs": [],
   "source": [
    "embeddings = get_embeddings(tokens, vec)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83c93e",
   "metadata": {},
   "source": [
    "There it is, the expected (N, L, D) shape. Let's take a quick look at the embeddings themselves. Just run the code below as is to visualize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5c150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971bc238",
   "metadata": {},
   "source": [
    "At the end of each tensor (in the first dimension, there are four of them), you'll see a bunch of zeros. These correspond to the padding tokens that are unknown to GloVe embeddings.\n",
    "\n",
    "It looks like a waste of space and computation to handle all these zero embeddings, right? As it turns out, these can either be ignored (by using masks that identify which tokens are meaningful - more on that later), or they can be completely dismissed at a much earlier stage, which brings us to the second alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29df743a",
   "metadata": {
    "id": "e795ea8e"
   },
   "source": [
    "#### 15.10.2.2 Alternative 2: Bag of Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2d5d06",
   "metadata": {},
   "source": [
    "The main purpose of padding sequences is to get matching lengths for all of them, after all, our models can only handle neatly organized tensors as inputs.\n",
    "\n",
    "But, what if we could get a single, neatly organized, tensor directly out of the sequence? One way to accomplish this is to simply compute the embeddings for each token in a sequence, regardless of how long the sequence actually is, and then aggregate all these tensors together by averaging them. That's called a bag of embeddings (BoE), and PyTorch even offers a special layer for it (`nn.EmbeddingBag`) that does the whole thing.\n",
    "\n",
    "The result, in this case, is a single tensor, with as many elements as the dimensionality of our vector (50, in the case of our GloVe), for each sentence. In this approach, it doesn't make sense to pad the sequences, otherwise we would be lowering the average by introducing a lot of zeros.\n",
    "\n",
    "Let's try this approach out! First, we retrieve the embeddings corresponding to the tokens in a given sentence. Just run the code below as is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b27726",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73d1f5c0",
    "outputId": "071e8c5c-44b1-47f0-8172-1c18fd251a23"
   },
   "outputs": [],
   "source": [
    "embeddings = vec.get_vecs_by_tokens(tokens[0])\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daea33f0",
   "metadata": {},
   "source": [
    "We'll get as many vectors back as there are tokens in the first sentence. Let's average them. Just run the code below as is to compute the average embedding for the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3888493",
   "metadata": {},
   "outputs": [],
   "source": [
    "boe = embeddings.mean(axis=0)\n",
    "boe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86971233",
   "metadata": {},
   "source": [
    "That's it, a single tensor of average embeddings. Easy, right?\n",
    "\n",
    "Now, write a function that takes as arguments:\n",
    "- a list of lists of tokens\n",
    "- an instance of `Vectors` (such as our own GloVe)\n",
    "\n",
    "It must retrieve the embeddings for the tokens in each inner list, average them, and concatenate the results together, so the resulting tensor to be returned has the shape (N, D):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc784627",
   "metadata": {
    "id": "10b3fe1a"
   },
   "outputs": [],
   "source": [
    "def get_bag_of_embeddings(tokens, vec):\n",
    "    # Retrieve embeddings from the Vector object using `get_vecs_by_tokens`\n",
    "    # For every list of tokens, take the average of their embeddings\n",
    "    # Make sure to get the shapes right, and concatenate the tensors so\n",
    "    # the resulting shape is N, D    \n",
    "    # write your code here\n",
    "    embeddings = torch.cat([vec.get_vecs_by_tokens(s).mean(axis=0).unsqueeze(0) for s in tokens], dim=0)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9406a4af",
   "metadata": {},
   "source": [
    "Just run the code below as is to inspect the shape of the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dbc900",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5cc7af6",
    "outputId": "f5046d77-3b72-4c8f-c2f6-3f13d38c7dda"
   },
   "outputs": [],
   "source": [
    "boe = get_bag_of_embeddings(tokens, vec)\n",
    "boe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82afed4b",
   "metadata": {},
   "source": [
    "The bag of embeddings is surely much more easy to handle, so we're sticking with that in this lab. Later on, when using larger models such as BERT, we'll to back to using the first alternative, including padding and masking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd40de40",
   "metadata": {},
   "source": [
    "### 15.10.2.3 Datapipes and Data Loaders\n",
    "\n",
    "Moreover, recreate the \"raw\" datapipes and data loaders using a larger batch size this time. Don't forget to shuffle the training set.\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41ed389",
   "metadata": {
    "id": "14569c8d"
   },
   "outputs": [],
   "source": [
    "datapipes = {}\n",
    "# write your code here\n",
    "datapipes['train'] = create_raw_datapipe('train.csv').shuffle(buffer_size=125000)\n",
    "datapipes['test'] = create_raw_datapipe('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2efb24",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0be3cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {}\n",
    "# write your code here\n",
    "dataloaders['train'] = DataLoader(dataset=datapipes['train'], batch_size=32, shuffle=True)\n",
    "dataloaders['test'] = DataLoader(dataset=datapipes['test'], batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6f1864",
   "metadata": {
    "id": "52f4a217"
   },
   "source": [
    "### 15.10.3 Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6bc450",
   "metadata": {},
   "source": [
    "Before writing the training loop itself, you need to:\n",
    "- create a model that's able to take a batch of bags of embeddings as inputs, and produce four logits as outputs (we suggest to keep it as simple as a single linear layer, but you're welcome to try more-complex models)\n",
    "- create an appropriate loss function for multi-class classification\n",
    "- create an optimizer to handle the model's parameters\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcbcca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(11)\n",
    "# write your code here\n",
    "model = nn.Sequential(nn.Linear(50, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d559429e",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5826cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3865eccd",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bf90e5",
   "metadata": {
    "id": "eceb84f7"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Suggested learning rate\n",
    "lr = 1e-3\n",
    "# write your code here\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40eb0d9",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step4.png)\n",
    "\n",
    "Finally, you may write the training loop. It is mostly the typical stuff we've done time and again, but remember that your mini-batches are tuples of `(labels, sentences)`, and you have to tokenize the sentences, and compute their corresponding bags of embeddings before feeding them to the model. You may leverage the functions you've already wrote to easily accomplish that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eddc5d9",
   "metadata": {
    "id": "0f5ec369"
   },
   "outputs": [],
   "source": [
    "vec = GloVe(name='6B', dim=50)\n",
    "\n",
    "batch_losses = []\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "## Training\n",
    "for i, batch in enumerate(dataloaders['train']):\n",
    "    # Set the model's mode\n",
    "    # write your code here\n",
    "    model.train()\n",
    "\n",
    "    # Unpack your batch (it has labels and sentences)\n",
    "    # Tokenize the sentences, and compute their bags of embeddings\n",
    "    # write your code here\n",
    "    labels, sentences = batch\n",
    "    tokens = tokenize_batch(sentences)\n",
    "    embeddings = get_bag_of_embeddings(tokens, vec)\n",
    "\n",
    "    embeddings = embeddings.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Step 1 - forward pass\n",
    "    # write your code here\n",
    "    predictions = model(embeddings)\n",
    "\n",
    "    # Step 2 - computing the loss\n",
    "    # write your code here\n",
    "    loss = loss_fn(predictions, labels)\n",
    "    \n",
    "    # Step 3 - computing the gradients\n",
    "    # write your code here\n",
    "    loss.backward()\n",
    "    \n",
    "    batch_losses.append(loss.item())\n",
    "\n",
    "    # Step 4 - updating parameters and zeroing gradients\n",
    "    # write your code here\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8745a7fd",
   "metadata": {},
   "source": [
    "It shouldn't take long to train this model (if you followed our suggestion to keep it as simple as it can be, that is). Just run the code below as is to visualize the losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f26837c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "1eb928b1",
    "outputId": "712a2d0d-d773-47e3-f846-d7c0223dac3d"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(batch_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea93ca47",
   "metadata": {},
   "source": [
    "### 15.10.4 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4116a41d",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step5.png)\n",
    "\n",
    "Losses are looking ok-ish, how about actual metrics? Let's use HuggingFace's `evaluate` package once again. This time, though, we're loading each metric (precision, recall, and accuracy) separately because we're dealing with a multi-class classification task, and this doesn't sit well with the `combine()` method (at the time of writing). Just run the code below as is to create evaluators for the three metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3c7f91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "471e186c18de4383b15df122dde6a575",
      "324d7db037b14f2f86a3a8d46e165bd3",
      "80fb9d4a84214a938dc20ebd4a3bd0db",
      "5ceec8c0bd31404997137109320b4ef9",
      "e621214eb0214d4c864302d9af7af9a7",
      "95e09d5079a34dd9adcb43b2757c614f",
      "f121587efd404e8c84046454530242ad",
      "e7861cdcdede4c02a196d638381177f7",
      "e6b18ddc07e34b1db05ddaaf764aa1f0",
      "1a8907be69cc42f0a1a3a71966a17484",
      "697d6d3fbc394e73a08053e3864f13ff",
      "96c5372621bf47baa2a3d6aa729b4f6b",
      "0b2b33bf2f0c4d5e8bceede3adf1ef97",
      "0384c4cc15b4455287528080092ce289",
      "611faf8344c34b4abdd00aae3d30ded2",
      "c520108f3af94100a71282f41d497183",
      "3d40a621ecb246a5b65ec8645af0cdf8",
      "f3a7de1633704a258af44901c06fe596",
      "201dc63725e74923af56c492a58e5738",
      "0e9c266058f8453e9ea06ee9267104f6",
      "37202d89fac148f5905f1820e2638cce",
      "48ab4f35ea3e4801bcdb7dbea07a57f4",
      "561ff46e2668414fb72a814d2b4e0cc4",
      "c495a52e8a92428ab41213b4751cdf55",
      "4d0326a1825a487c87e8583825e44bfd",
      "b98f84758cc848d5af3c1ac8eaa6ff80",
      "291d5222e35b4718a6998d6d6e974896",
      "e21cfec838b047fcb61ed00df08f72a4",
      "a2ac1015d8014751b716130ec0f70fd4",
      "e40104eeedd4455184d352a63606a55f",
      "a06ddfe81de54179a32ff11c0c944bc1",
      "a6ccabd302134e13ae1bff890097d11d",
      "2cde1680ac044bf78947b26f104629c5"
     ]
    },
    "id": "4db38c6f",
    "outputId": "1880e96c-b629-4e19-9885-6d4c99487b69"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric1 = evaluate.load('precision', average=None)\n",
    "metric2 = evaluate.load('recall', average=None)\n",
    "metric3 = evaluate.load('accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6798930d",
   "metadata": {},
   "source": [
    "Write an evaluation loop that goes over the mini-batches in the test data pipe and:\n",
    "- tokenizes the sentences\n",
    "- retrieves their corresponding bags of embeddings\n",
    "- get predictions from the model (logits)\n",
    "- gets the most-likely class from the logits\n",
    "- adds both predicted classes and labels to the metrics objects we've just created using their `add_batch()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdad68d6",
   "metadata": {
    "id": "2db9dfd7"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "for batch in dataloaders['test']:\n",
    "    # Unpack your batch (it has labels and sentences)\n",
    "    # Tokenize the sentences, and compute their bags of embeddings\n",
    "    # write your code here\n",
    "    labels, sentences = batch\n",
    "    tokens = tokenize_batch(sentences)\n",
    "    embeddings = get_bag_of_embeddings(tokens, vec)\n",
    "        \n",
    "    embeddings = embeddings.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # write your code here\n",
    "    predictions = model(embeddings)\n",
    "\n",
    "    # write your code here\n",
    "    pred_class = predictions.argmax(dim=1)\n",
    "    \n",
    "    pred_class = pred_class.tolist()\n",
    "    labels = labels.tolist()\n",
    "\n",
    "    metric1.add_batch(references=labels, predictions=pred_class)\n",
    "    metric2.add_batch(references=labels, predictions=pred_class)\n",
    "    metric3.add_batch(references=labels, predictions=pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6bb21d",
   "metadata": {},
   "source": [
    "Finally, call each metric's `compute()` object to get the results. Just run the code below as is to visualize the resulting metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42752643",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0085caf",
    "outputId": "578ead97-ee02-4a7a-adbf-ea0b64ee4bb9"
   },
   "outputs": [],
   "source": [
    "metric1.compute(average=None), metric2.compute(average=None), metric3.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15813022",
   "metadata": {},
   "source": [
    "A single linear layer can achieve roughly 85% accuracy, which isn't bad at all! Even old, traditional, embeddings such as GloVe can lead to pretty decent results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
