{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Instructions\n",
    "\n",
    "In the lab, you're presented a task such as building a dataset, training a model, or writing a training loop, and we'll provide the code structured in such a way that you can fill in the blanks in the code using the knowledge you acquired in the chapters that precede the lab. You should be able to find appropriate snippets of code in the course content that work well in the lab with minor or no adjustments.\n",
    "\n",
    "The blanks in the code are indicated by ellipsis (`...`) and comments (`# write your code here`).\n",
    "\n",
    "In some cases, we'll provide you partial code to ensure the right variables are populated and any code that follows it runs accordingly.\n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "x = ...\n",
    "```\n",
    "\n",
    "The solution should be a single statement that replaces the ellipsis, such as:\n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "x = [0, 1, 2]\n",
    "```\n",
    "\n",
    "In some other cases, when there is no new variable being created, the blanks are shown like in the example below: \n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "...\n",
    "```\n",
    "\n",
    "Although we're showing you only a single ellipsis (`...`), you may have to write more than one line of code to complete the step, such as:\n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "for i, xi in enumerate(x):\n",
    "    x[i] = xi * 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation Notes\n",
    "\n",
    "To run this notebook on Google Colab, you will need to install the following libraries: transformers, evaluate, and datasets.\n",
    "\n",
    "In Google Colab, you can run the following command to install these libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers evaluate datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1f1a96f"
   },
   "source": [
    "## 15.10 Lab 6: Text Classification using Embeddings\n",
    "\n",
    "It is time to get our hands dirty! Let's use GloVe pretrained word embeddings as features for a multi-class linear classification model. It works like a linear regression model, but it produces four logits as output (one for each class in the AG News Dataset), and we'll use the softmax function to convert the logits into probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.10.1 Recap\n",
    "\n",
    "In the last chapter, we loaded the AG News Dataset, cleaned it up of special characters and HTML tags, and discarded the title information, returning only labels and (cleaned) descriptions. Let's quickly retrace our steps here to prepare the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step1.png)\n",
    "\n",
    "First, we need to download the dataset. You can dowload the files from the following links:\n",
    "- `https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv`\n",
    "- `https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv`\n",
    "- `https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/classes.txt`\n",
    "\n",
    "Alternatively, you can download all files as a single compressed file instead:\n",
    "\n",
    "```\n",
    "https://github.com/dvgodoy/assets/raw/main/PyTorchInPractice/data/AGNews/agnews.zip\n",
    "```\n",
    "\n",
    "If you're running Google Colab, you can download the files using the commands below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11466c94"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
    "!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv\n",
    "!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/classes.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step2.png)\n",
    "\n",
    "Next, let's do some data cleaning, getting rid of a few HTML tags, replacing some special characters, etc. Here is a non-exhaustive list of characters and tags for replacement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7c8cbebf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "chr_codes = np.array([\n",
    "     36,   151,    38,  8220,   147,   148,   146,   225,   133,    39,  8221,  8212,   232,   149,   145,   233,\n",
    "  64257,  8217,   163,   160,    91,    93,  8211,  8482,   234,    37,  8364,   153,   195,   169\n",
    "])\n",
    "chr_subst = {f' #{c};':chr(c) for c in chr_codes}\n",
    "chr_subst.update({' amp;': '&', ' quot;': \"'\", ' hellip;': '...', ' nbsp;': ' ', '&lt;': '', '&gt;': '',\n",
    "                  '&lt;em&gt;': '', '&lt;/em&gt;': '', '&lt;strong&gt;': '', '&lt;/strong&gt;': ''})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are a couple of helper functions we used to perform the cleanup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2108e26"
   },
   "outputs": [],
   "source": [
    "def replace_chars(sent):\n",
    "    to_replace = [c for c in list(chr_subst.keys()) if c in sent]\n",
    "    for c in to_replace:\n",
    "        sent = sent.replace(c, chr_subst[c])\n",
    "    return sent\n",
    "\n",
    "def preproc_description(desc):\n",
    "    desc = desc.replace('\\\\', ' ').strip()\n",
    "    return replace_chars(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " After loading the CSV files using `load_dataset()` and building a `DatasetDict` out of them, we used the functions above to transform our datasets, cleaning up the text and converting the label into a 0-based numeric value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Split, DatasetDict\n",
    "\n",
    "colnames = ['topic', 'title', 'news']\n",
    "\n",
    "train_ds = load_dataset(\"csv\", data_files='train.csv', sep=',', split=Split.ALL, column_names=colnames)\n",
    "test_ds = load_dataset(\"csv\", data_files='test.csv', sep=',', split=Split.ALL, column_names=colnames)\n",
    "\n",
    "datasets = DatasetDict({'train': train_ds, 'test': test_ds})\n",
    "datasets = datasets.map(lambda row: {'topic': row['topic']-1, 'news': preproc_description(row['news'])})\n",
    "datasets = datasets.select_columns(['topic', 'news'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.10.2 Tokenizing and Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plan ahead what needs to be done:\n",
    "- create data loaders, one for each split\n",
    "- write a function that tokenizes the sentences in a given batch\n",
    "- write a function that converts tokens into token ids for every sentence in a given batch\n",
    "- retrieve the word embeddings for each and every token\n",
    "- create a linear model that takes the embedding vectors as features\n",
    "- create the appropriate loss function and optimizer\n",
    "- write a training loop\n",
    "\n",
    "Create two data loaders, one for each split (training and validation/test). For now, use a small batch size, such as four, to be able to more easily peek at the values. Later on, you'll recreate the data loader with a more appropriate batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e934f5e5"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloaders = {}\n",
    "# write your code here\n",
    "dataloaders['train'] = DataLoader(dataset=datasets['train'], batch_size=4, shuffle=True)\n",
    "dataloaders['test'] = DataLoader(dataset=datasets['test'], batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch one mini-batch of data to make sure it's working fine. Just run the code below as is to visualize the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e35f7f98",
    "outputId": "2654e182-0026-4286-db16-e33360c546fa"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(dataloaders['train']))\n",
    "labels, sentences = batch['topic'], batch['news']\n",
    "labels, sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step3.png)\n",
    "\n",
    "Now, write a function that tokenizes a mini-batch of sentences. The function must take as arguments:\n",
    "- a tuple or list containing multiple sentences (as returned by the data loader)\n",
    "- an optional tokenizer: if the tokenizer isn't provided, it should fall back to the default  `simple_preprocess()` function we have used before\n",
    "\n",
    "The function must return a list of lists of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76582d02"
   },
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def tokenize_batch(sentences, tokenizer=None):\n",
    "    # Create the basic tokenizer if one isn't provided\n",
    "    # write your code here\n",
    "    if tokenizer is None:\n",
    "        tokenizer = simple_preprocess\n",
    "    \n",
    "    # Tokenize sentences and returns the result\n",
    "    # write your code here\n",
    "    return [tokenizer(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try your function out and assign its output to the `tokens` variable. Just run the code below as is to visualize the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cede23a5",
    "outputId": "4ad42833-7d73-4152-e28e-998a788f61a2"
   },
   "outputs": [],
   "source": [
    "tokens = tokenize_batch(sentences)\n",
    "for v in tokens:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More likely than not, each sentence in a mini-batch has different number of tokens in it. How many tokens are there in each sentence? Just run the code below as is to see the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8da1218",
    "outputId": "b5a01f17-d77c-4387-89a9-689569916055"
   },
   "outputs": [],
   "source": [
    "[len(s) for s in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's briefly discuss two different approaches to handling this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43280d03"
   },
   "source": [
    "#### 15.10.2.1 Alternative 1: Padding\n",
    "\n",
    "Did padding come to your mind? We have taken this approach time and again.\n",
    "\n",
    "Now, you'll write a function called `encode_batch()` that combines both truncating and padding operations. The function must take as arguments:\n",
    "- a vocabulary dictionary, mapping tokens/words to their corresponding indices\n",
    "- a list of lists of tokens (as returned by the `tokenize_batch()` function)\n",
    "- the maximum length of tokens, above which they are truncated\n",
    "- an optional boolean argument indicating if the sequences should be padded\n",
    "- an optional id for the padding token (e.g. -1)\n",
    "- an optional id for the unknown token (e.g. -1)\n",
    "\n",
    "The function must truncate sequences of tokens that are too long and, afterward, pad the sequences so the shorter ones match the length of the longest.\n",
    "\n",
    "It must return a list of lists of token ids, every inner list having the same length.\n",
    "\n",
    "We're loading Gensim's GloVe embeddings, so you may use its `key_to_index()` method as the vocabulary dictionary. You can also call the `encode_str()` function from Chapter 15 to convert words/tokens into their corresponding ids.\n",
    "\n",
    "Perhaps you've also noticed that the default values for both padding and unknown tokens are the same. We'll keep them like that for now, but we'll assign them other values shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import downloader\n",
    "\n",
    "vec = downloader.load('glove-wiki-gigaword-50')\n",
    "\n",
    "def encode_str(key_to_index, tokens, unk_token=-1):\n",
    "    token_ids = [key_to_index.get(token, unk_token) for token in tokens]\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_batch(key_to_index, batch, max_len=None, padding=False, pad_token_id=-1, unk_token_id=-1):\n",
    "    # Truncate every sentence to max_len\n",
    "    # write your code here\n",
    "    if isinstance(max_len, int):\n",
    "        truncated = [s[:max_len] for s in batch]\n",
    "    else:\n",
    "        truncated = batch[:]\n",
    "\n",
    "    # Check the actual maximum length of the (truncated) inputs\n",
    "    # write your code here\n",
    "    current_max = max([len(s) for s in truncated])\n",
    "    \n",
    "    batch_ids = []\n",
    "    for tokens in truncated:\n",
    "        token_ids = encode_str(key_to_index, tokens, unk_token_id)\n",
    "        if padding:\n",
    "            # Appends as many padding tokens as necessary to make every\n",
    "            # sentence as long as the actual maximum length\n",
    "            # write your code here            \n",
    "            token_ids.extend([pad_token_id] * (current_max - len(token_ids)))\n",
    "        batch_ids.append(token_ids)\n",
    "    return batch_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encode_batch(vec.key_to_index, tokens, padding=True, max_len=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-check that every inner list has the same length, as expected. Just run the code below as is to visualize the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "d44fb5fc",
    "outputId": "6dd864ed-d386-475f-8656-98a1f301a9f3"
   },
   "outputs": [],
   "source": [
    "padded_token_ids = encode_batch(vec.key_to_index, tokens, padding=True)\n",
    "lengths = [len(s) for s in padded_token_ids]\n",
    "lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same length everywhere? Great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we try retrieving the embeddings for the padded sequences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "tensor_glove = torch.as_tensor(vec.vectors).float()\n",
    "embedding = nn.Embedding.from_pretrained(tensor_glove)\n",
    "\n",
    "def get_embeddings(embedding, token_ids):\n",
    "    valid_ids = torch.as_tensor([token_id for token_id in token_ids if token_id >= 0])\n",
    "    embedded_tokens = embedding(valid_ids)\n",
    "    return embedded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(padded_token_ids[0]))\n",
    "print(padded_token_ids[0])\n",
    "print(get_embeddings(embedding, padded_token_ids[0]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shouldn't be a surprise that the embeddings are shorter than the padded sequence, after all, we are explicitly filtering out invalid ids in the `get_embeddings()` function. We have to make one small change to our embeddings to account for the possibility of padding and unknown tokens.\n",
    "\n",
    "So, we are appending not one, but two tensors full of zeros to our embeddings, one for the padding token and another one for the unknown token. We're setting the pad token id to the index corresponding to the second-to-last entry in the embedding layer, and the unknown token to the entry after that. This way, the mappings (from word/token to index) are preserved, and the only difference is that we call the `encode_batch()` function using different padding and unknown token ids this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_glove = torch.as_tensor(vec.vectors).float()\n",
    "tensor_glove = torch.cat([tensor_glove, torch.zeros((2, vec.vector_size))])\n",
    "\n",
    "embedding = nn.Embedding.from_pretrained(tensor_glove)\n",
    "# padding and unknown tokens are the last ones, so we don't mess with the key_to_index\n",
    "pad_token_id = embedding.num_embeddings - 2\n",
    "unk_token_id = pad_token_id + 1\n",
    "\n",
    "padded_token_ids = encode_batch(vec.key_to_index, tokens, padding=True, pad_token_id=pad_token_id, unk_token_id=unk_token_id)\n",
    "\n",
    "get_embeddings(embedding, padded_token_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, right? Now, we can retrieve embeddings using the same `get_embeddings()` function whether our sequences are padded or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's next? Let's write a function that takes as arguments:\n",
    "- an embedding layer\n",
    "- a list of lists of token ids\n",
    "\n",
    "And retrieves the corresponding embeddings for the whole batch as a tensor in the shape (N, L, D) where:\n",
    "- N is the number of data points in a mini-batch\n",
    "- L is the number of tokens in each sequence (they all have the same length now)\n",
    "- D is the number of dimensions in each embedding vector (50 in our instance of GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_embeddings(embedding, token_ids):\n",
    "    # Retrieve embeddings from the embedding layer using the token ids\n",
    "    # Make sure to get the shapes right, and concatenate the tensors so\n",
    "    # the resulting shape is N, L, D\n",
    "    # write your code here\n",
    "    embeddings = torch.cat([get_embeddings(embedding, ids).unsqueeze(0)\n",
    "                            for ids in token_ids], dim=0)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just run the code below as is to inspect the shape of the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88af0fef",
    "outputId": "aab558ba-29e5-42e1-a080-461fcd1d0b90"
   },
   "outputs": [],
   "source": [
    "token_ids = encode_batch(vec.key_to_index, tokens, padding=True, pad_token_id=pad_token_id, unk_token_id=unk_token_id)\n",
    "embeddings = get_batch_embeddings(embedding, token_ids)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There it is, the expected (N, L, D) shape. Let's take a quick look at the embeddings themselves. Just run the code below as is to visualize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of each tensor (in the first dimension, there are four of them), you'll see a bunch of zeros. These correspond to the padding tokens that we appended to GloVe embeddings.\n",
    "\n",
    "It looks like a waste of space and computation to handle all these zero embeddings, right? As it turns out, these can either be ignored (by using masks that identify which tokens are meaningful - more on that later), or they can be completely dismissed at a much earlier stage, which brings us to the second alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e795ea8e"
   },
   "source": [
    "#### 15.10.2.2 Alternative 2: Bag of Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main purpose of padding sequences is to get matching lengths for all of them, after all, our models can only handle neatly organized tensors as inputs.\n",
    "\n",
    "But, what if we could get a single, neatly organized, tensor directly out of the sequence? One way to accomplish this is to simply compute the embeddings for each token in a sequence, regardless of how long the sequence actually is, and then aggregate all these tensors together by averaging them. That's called a bag of embeddings (BoE), and PyTorch even offers a special layer for it (`nn.EmbeddingBag`) that does the whole thing.\n",
    "\n",
    "The result, in this case, is a single tensor, with as many elements as the dimensionality of our vector (50, in the case of our GloVe), for each sentence. In this approach, it doesn't make sense to pad the sequences, otherwise we would be lowering the average by introducing a lot of zeros.\n",
    "\n",
    "Let's try this approach out! First, we retrieve the embeddings corresponding to the tokens in a given sentence. Just run the code below as is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73d1f5c0",
    "outputId": "071e8c5c-44b1-47f0-8172-1c18fd251a23"
   },
   "outputs": [],
   "source": [
    "token_ids = encode_batch(vec.key_to_index, tokens, padding=False)\n",
    "embeddings = get_embeddings(embedding, token_ids[0])\n",
    "\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll get as many vectors back as there are tokens in the first sentence. Let's average them. Just run the code below as is to compute the average embedding for the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boe = embeddings.mean(axis=0)\n",
    "boe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, a single tensor of average embeddings. Easy, right?\n",
    "\n",
    "Now, write a function that takes as arguments:\n",
    "- an embedding layer\n",
    "- a list of lists of token ids\n",
    "\n",
    "It must retrieve the embeddings for the tokens in each inner list, average them, and concatenate the results together, so the resulting tensor to be returned has the shape (N, D):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10b3fe1a"
   },
   "outputs": [],
   "source": [
    "def get_bag_of_embeddings(embedding, token_ids):\n",
    "    # Retrieve embeddings from the embedding layer using the token ids\n",
    "    # For every list of tokens, take the average of their embeddings\n",
    "    # Make sure to get the shapes right, and concatenate the tensors so\n",
    "    # the resulting shape is N, D    \n",
    "    # write your code here\n",
    "    embeddings = torch.cat([get_embeddings(embedding, ids).mean(axis=0).unsqueeze(0) \n",
    "                            for ids in token_ids], dim=0)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just run the code below as is to inspect the shape of the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5cc7af6",
    "outputId": "f5046d77-3b72-4c8f-c2f6-3f13d38c7dda"
   },
   "outputs": [],
   "source": [
    "token_ids = encode_batch(vec.key_to_index, tokens, padding=False)\n",
    "boe = get_bag_of_embeddings(embedding, token_ids)#, vec)\n",
    "boe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag of embeddings is surely much more easy to handle, so we're sticking with that in this lab. Later on, when using larger models such as BERT, we'll to back to using the first alternative, including padding and masking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.10.2.3 Data Loaders\n",
    "\n",
    "Moreover, recreate the data loaders using a larger batch size this time.\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {}\n",
    "# write your code here\n",
    "dataloaders['train'] = DataLoader(dataset=datasets['train'], batch_size=32, shuffle=True)\n",
    "dataloaders['test'] = DataLoader(dataset=datasets['test'], batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52f4a217"
   },
   "source": [
    "### 15.10.3 Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before writing the training loop itself, you need to:\n",
    "- create a model that's able to take a batch of bags of embeddings as inputs, and produce four logits as outputs (we suggest to keep it as simple as a single linear layer, but you're welcome to try more-complex models)\n",
    "- create an appropriate loss function for multi-class classification\n",
    "- create an optimizer to handle the model's parameters\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(11)\n",
    "# write your code here\n",
    "model = nn.Sequential(nn.Linear(50, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eceb84f7"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Suggested learning rate\n",
    "lr = 1e-3\n",
    "# write your code here\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step4.png)\n",
    "\n",
    "Finally, you may write the training loop. It is mostly the typical stuff we've done time and again, but remember that your mini-batches are dictionaries, and you have to tokenize and encode (that is, converting tokens into token ids) the sentences, and compute their corresponding bags of embeddings before feeding them to the model. You may leverage the functions you've already wrote to easily accomplish that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0f5ec369"
   },
   "outputs": [],
   "source": [
    "vec = downloader.load('glove-wiki-gigaword-50')\n",
    "\n",
    "tensor_glove = torch.as_tensor(vec.vectors).float()\n",
    "# we don't need to bother appending zero tensors for padding and unknown tokens\n",
    "# since we're using a bag of embeddings, that is, we simply average the valid\n",
    "# tokens only and ignore the rest.\n",
    "embedding = nn.Embedding.from_pretrained(tensor_glove)\n",
    "\n",
    "batch_losses = []\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "## Training\n",
    "for i, batch in enumerate(dataloaders['train']):\n",
    "    # Set the model's mode\n",
    "    # write your code here\n",
    "    model.train()\n",
    "\n",
    "    # Unpack your batch (it has labels and sentences)\n",
    "    # Tokenize the sentences, and compute their bags of embeddings\n",
    "    # write your code here\n",
    "    #labels, sentences = batch\n",
    "    labels, sentences = batch['topic'], batch['news']\n",
    "    tokens = tokenize_batch(sentences)\n",
    "    token_ids = encode_batch(vec.key_to_index, tokens, padding=False)\n",
    "    embeddings = get_bag_of_embeddings(embedding, token_ids)\n",
    "\n",
    "    embeddings = embeddings.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Step 1 - forward pass\n",
    "    # write your code here\n",
    "    predictions = model(embeddings)\n",
    "\n",
    "    # Step 2 - computing the loss\n",
    "    # write your code here\n",
    "    loss = loss_fn(predictions, labels)\n",
    "    \n",
    "    # Step 3 - computing the gradients\n",
    "    # write your code here\n",
    "    loss.backward()\n",
    "    \n",
    "    batch_losses.append(loss.item())\n",
    "\n",
    "    # Step 4 - updating parameters and zeroing gradients\n",
    "    # write your code here\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shouldn't take long to train this model (if you followed our suggestion to keep it as simple as it can be, that is). Just run the code below as is to visualize the losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "1eb928b1",
    "outputId": "712a2d0d-d773-47e3-f846-d7c0223dac3d"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(batch_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.10.4 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step5.png)\n",
    "\n",
    "Losses are looking ok-ish, how about actual metrics? Let's use HuggingFace's `evaluate` package once again. This time, though, we're loading each metric (precision, recall, and accuracy) separately because we're dealing with a multi-class classification task, and this doesn't sit well with the `combine()` method (at the time of writing). Just run the code below as is to create evaluators for the three metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "471e186c18de4383b15df122dde6a575",
      "324d7db037b14f2f86a3a8d46e165bd3",
      "80fb9d4a84214a938dc20ebd4a3bd0db",
      "5ceec8c0bd31404997137109320b4ef9",
      "e621214eb0214d4c864302d9af7af9a7",
      "95e09d5079a34dd9adcb43b2757c614f",
      "f121587efd404e8c84046454530242ad",
      "e7861cdcdede4c02a196d638381177f7",
      "e6b18ddc07e34b1db05ddaaf764aa1f0",
      "1a8907be69cc42f0a1a3a71966a17484",
      "697d6d3fbc394e73a08053e3864f13ff",
      "96c5372621bf47baa2a3d6aa729b4f6b",
      "0b2b33bf2f0c4d5e8bceede3adf1ef97",
      "0384c4cc15b4455287528080092ce289",
      "611faf8344c34b4abdd00aae3d30ded2",
      "c520108f3af94100a71282f41d497183",
      "3d40a621ecb246a5b65ec8645af0cdf8",
      "f3a7de1633704a258af44901c06fe596",
      "201dc63725e74923af56c492a58e5738",
      "0e9c266058f8453e9ea06ee9267104f6",
      "37202d89fac148f5905f1820e2638cce",
      "48ab4f35ea3e4801bcdb7dbea07a57f4",
      "561ff46e2668414fb72a814d2b4e0cc4",
      "c495a52e8a92428ab41213b4751cdf55",
      "4d0326a1825a487c87e8583825e44bfd",
      "b98f84758cc848d5af3c1ac8eaa6ff80",
      "291d5222e35b4718a6998d6d6e974896",
      "e21cfec838b047fcb61ed00df08f72a4",
      "a2ac1015d8014751b716130ec0f70fd4",
      "e40104eeedd4455184d352a63606a55f",
      "a06ddfe81de54179a32ff11c0c944bc1",
      "a6ccabd302134e13ae1bff890097d11d",
      "2cde1680ac044bf78947b26f104629c5"
     ]
    },
    "id": "4db38c6f",
    "outputId": "1880e96c-b629-4e19-9885-6d4c99487b69"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric1 = evaluate.load('precision', average=None)\n",
    "metric2 = evaluate.load('recall', average=None)\n",
    "metric3 = evaluate.load('accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write an evaluation loop that goes over the mini-batches in the test data pipe and:\n",
    "- tokenizes the sentences\n",
    "- encode the sentences (convert their tokens into token ids)\n",
    "- retrieves their corresponding bags of embeddings\n",
    "- get predictions from the model (logits)\n",
    "- gets the most-likely class from the logits\n",
    "- adds both predicted classes and labels to the metrics objects we've just created using their `add_batch()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2db9dfd7"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "for batch in dataloaders['test']:\n",
    "    # Unpack your batch (it has labels and sentences)\n",
    "    # Tokenize and encode the sentences, and compute their bags of embeddings\n",
    "    # write your code here\n",
    "    labels, sentences = batch['topic'], batch['news']\n",
    "    tokens = tokenize_batch(sentences)\n",
    "    token_ids = encode_batch(vec.key_to_index, tokens, padding=False)\n",
    "    embeddings = get_bag_of_embeddings(embedding, token_ids)\n",
    "        \n",
    "    embeddings = embeddings.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # write your code here\n",
    "    predictions = model(embeddings)\n",
    "\n",
    "    # write your code here\n",
    "    pred_class = predictions.argmax(dim=1)\n",
    "    \n",
    "    pred_class = pred_class.tolist()\n",
    "    labels = labels.tolist()\n",
    "\n",
    "    metric1.add_batch(references=labels, predictions=pred_class)\n",
    "    metric2.add_batch(references=labels, predictions=pred_class)\n",
    "    metric3.add_batch(references=labels, predictions=pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, call each metric's `compute()` object to get the results. Just run the code below as is to visualize the resulting metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0085caf",
    "outputId": "578ead97-ee02-4a7a-adbf-ea0b64ee4bb9"
   },
   "outputs": [],
   "source": [
    "metric1.compute(average=None), metric2.compute(average=None), metric3.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single linear layer can achieve roughly 85% accuracy, which isn't bad at all! Even old, traditional, embeddings such as GloVe can lead to pretty decent results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
